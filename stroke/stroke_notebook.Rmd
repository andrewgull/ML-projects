---
title: "Stroke prediction"
author: "AG"
date: "last update: `r format(Sys.Date(), format = '%d %B %Y')`"
output:
  html_document:
    toc: yes
    number_sections: true
    df_print: paged
    code_folding: hide
    fig_width: 10
    fig_height: 6
    theme: cerulean
    highlight: espresso
    toc_float: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F, cache = F)
library(tidymodels)
library(skimr)
library(vip)
library(probably)
library(themis)
```

# Data set

```{r read}
# upon quick look at the data set
# if you set smoking_status to factor in col_types, na() won't work
# remove ID, Sex == Other
# output to a factor
df <- 
  readr::read_csv("data/healthcare-dataset-stroke-data.csv", col_types = "cfdfffffddcf", na = c("Unknown", "N/A")) %>% 
  mutate(smoking_status = factor(smoking_status),
         stroke = factor(ifelse(stroke == 1, "yes", "no"), levels = c("yes", "no"))) %>% 
  select(-id)

df
```

Further changes to the data set:

- In `smoking_status` 'Unknown' should be changed to NA.

- Also, it can be ordered: never < formerly < smokes

- only `hypertension` and `heart_disease` are dummies.

Other columns seem to be OK

## Descriptive statistics

### Factors

```{r describe}
skim(df) %>%
  yank("factor")
```

- Target 'stroke' is hugely imbalanced!

- 'smoking_status' completeness rate is low

 - One 'Other' gender can be removed


### Numerics

```{r filter}
df <- df %>% filter(gender != "Other")

skim(df) %>%
  yank("numeric")
```

- BMI's complete rate 0.96

### How many `smoking_status` in each target class?

Keep in mind that if smoking NAs are mainly in "healthy" class, we can simply remove them

```{r}
df %>% group_by(stroke, smoking_status) %>% 
  count()
```

A lot of them are in "stroke" group, some imputation will be needed.

### How many skipped BMI values in each target class?

```{r}
df %>% filter(is.na(bmi)) %>% 
  group_by(stroke) %>% 
  count()
```

- We have too many NAs in BMI among 'stroke-yes' cases to simply remove them. Some imputation is needed.

# Exploratory Data Analysis

## Quick overview

```{r pairs, fig.width=10, fig.height=10, warning=FALSE, message=FALSE}
GGally::ggpairs(df, aes(color = stroke, alpha = 0.2, dotsize = 0.02), 
        upper = list(continuous = GGally::wrap("cor", size = 2.5)),
        diag = list(continuous = "barDiag")) +
  scale_color_brewer(palette = "Set1", direction = -1) +
  scale_fill_brewer(palette = "Set1", direction = -1)
```

## In details

### Stroke vs Age

```{r}
ggplot(df, aes(stroke, age)) +
  geom_boxplot(aes(fill = stroke), alpha = 0.5, varwidth = T, notch = T) +
  geom_violin(aes(fill = stroke), alpha = 0.5) +
  scale_fill_brewer(palette = "Set1", direction = -1) +
  xlab("")
```

- No surprises here: the older you get the higher the chance of getting stroke.

- There are observation with age much below 20 y.o., even close to 0! 
These are very young kids or babies - should we even include them in the analysis?
If yes, the rest will be prediction only for adults. 

- Stroke in kids probably has very different causes compared to stroke in adults/older folk.

### Stroke vs Age + Gender

```{r box1}
ggplot(df, aes(stroke, age)) + 
  geom_violin(alpha=0.3) +
  geom_jitter(alpha=0.2, size=0.8, width = 0.15, height = 0.1, aes(color = gender)) + 
  geom_boxplot(alpha = 0.2) +
  scale_color_brewer(palette = "Set2", direction = -1)
```

- No gender imbalance with respect to `age` and `stroke`

### Stroke vs Glucose

```{r}
ggplot(df, aes(stroke, avg_glucose_level)) +
  geom_boxplot(aes(fill = stroke), alpha = 0.5, varwidth = T, notch = T) +
  geom_violin(aes(fill = stroke), alpha = 0.5) +
  scale_fill_brewer(palette = "Set1", direction = -1) +
  xlab("") +
  ylab("avg glucose level")
```

- Observations with stroke tend to have higher glucose level

This average glucose level is probably the results of fasting blood sugar test

If I correctly understand this [CDC information](https://www.cdc.gov/diabetes/basics/getting-tested.html#:~:text=Fasting%20Blood%20Sugar%20Test&text=A%20fasting%20blood%20sugar%20level,higher%20indicates%20you%20have%20diabetes.) on diabetes, values greater than 126 is evidence of diabetes.  But >250? Is it realistic?

### Stroke vs BMI

```{r}
ggplot(df, aes(stroke, bmi)) +
  geom_boxplot(aes(fill = stroke), alpha = 0.5, varwidth = T, notch = T) +
  geom_violin(aes(fill = stroke), alpha = 0.5) +
  scale_fill_brewer(palette = "Set1", direction = -1) +
  xlab("")
```

- BMI over 40 is the 3rd class of obesity - BMI over 75 should not exist at all, I think.

Let's look at this weird points

### Age vs BMI

```{r}
facet_names <- c("no" = "no stroke", "yes" = "stroke")

ggplot(df, aes(age, bmi)) +
  geom_point(color = "steelblue", alpha = 0.8, size = 0.5) +
  facet_grid(rows = vars(stroke), labeller = as_labeller(facet_names)) +
  guides()
```

Patients with BMI over 75 are also very young. Suspicious.

### Glucose vs Age + smoking

```{r}
ggplot(df, aes(age, avg_glucose_level)) +
  geom_point(aes(color = smoking_status), alpha = 0.6, size = 1) +
  scale_fill_brewer(palette = "Set1", direction = -1) +
  facet_grid(rows = vars(stroke), labeller = as_labeller(facet_names)) +
  guides()
```

- Children are mainly of 'Unknown' smoking status; both target groups are divided into two clusters -- I'm curious why.
It is not gender, nor heart disease or any other factor we have in the data set.

### Age vs Smoking

```{r}
ggplot(df, aes(smoking_status, age)) +
  geom_boxplot(aes(fill = stroke), alpha = 0.5, varwidth = T, notch = T) +
  scale_fill_brewer(palette = "Set1", direction = -1) +
  xlab("")
```

- Kids are main non-smokers of course. Note the same two outliers of age below 20 in 'stroke-yes'

### Glucose vs BMI

```{r}
ggplot(df, aes(avg_glucose_level, bmi)) +
  geom_point(aes(color = age), alpha = 0.6, size = 1) +
  scale_fill_brewer(palette = "Set1", direction = -1) +
  facet_grid(rows = vars(stroke), labeller = as_labeller(facet_names)) +
  guides() +
  xlab("avg glucose level")
```

- BMI outliers: super high BMI but super low glucose levels? How's that possible?

- Can it be a bias introduced by testing protocol misuse? Like instead of measuring glucose *before* eating, in some samples it was done *after* eating?

- Again, all the observations in both target classes form two distinct clusters.

### Stroke vs Gender

```{r bar1}
gender <- df %>% group_by(stroke, gender) %>% summarize(N=n())

ggplot(gender, aes(stroke, N)) +
  geom_bar(aes(fill=gender), alpha = 0.8, stat = "identity", position = "fill") +
  scale_fill_brewer(palette = "Set2", direction = -1) +
  ylab("proportion")
```

- No disproportions here

### Stroke vs Hypertension

```{r bar2}
hyptens <- df %>% group_by(stroke, hypertension) %>% summarize(N = n())

ggplot(hyptens, aes(stroke, N)) +
  geom_bar(aes(fill = hypertension), alpha = 0.8, stat = "identity", position = "fill") +
  scale_fill_brewer(palette = "Set2", direction = -1) +
  ylab("proportion")
```

- Hypertension occurred more often in 'stroke-yes'

### Stroke vs Heart Disease

```{r}
heart <- df %>% group_by(stroke, heart_disease) %>% summarize(N=n())

ggplot(heart, aes(stroke, N)) +
  geom_bar(aes(fill = heart_disease), alpha = 0.8, stat = "identity", position = "fill") +
  scale_fill_brewer(palette = "Set2", direction = 1) +
  ylab("proportion")
```

- Heart disease occurred more often in 'stroke-yes'

### Stroke vs Ever Married

```{r}
married <- df %>% group_by(stroke, ever_married) %>% summarize(N=n())

ggplot(married, aes(stroke, N)) +
  geom_bar(aes(fill = ever_married), alpha = 0.8, stat = "identity", position = "fill") +
  scale_fill_brewer(palette = "Set2", direction = -1) +
  ylab("proportion")
```

- Marriage is bad :)

### Stroke vs Work Type

```{r}
work <- df %>% group_by(stroke, work_type) %>% summarize(N=n())

ggplot(work, aes(stroke, N)) +
  geom_bar(aes(fill = work_type), alpha = 0.8, stat = "identity", position = "fill") +
  scale_fill_brewer(palette = "Set2", direction = 1) +
  ylab("proportion")
```

- It's good to be a child

- It's bad to be self-employed

### Stroke vs Residence Type

```{r}
residence <- df %>% group_by(stroke, Residence_type) %>% summarize(N=n())

ggplot(residence, aes(stroke, N)) +
  geom_bar(aes(fill = Residence_type), alpha = 0.8, stat = "identity", position = "fill") +
  scale_fill_brewer(palette = "Set2", direction = 1) +
  ylab("proportion")
```

- No disproportions here

### Stroke vs Smoking

```{r}
smoking <- df %>% group_by(stroke, smoking_status) %>% summarize(N=n())

ggplot(smoking, aes(stroke, N)) +
  geom_bar(aes(fill = smoking_status), alpha = 0.8, stat = "identity", position = "fill") +
  scale_fill_brewer(palette = "Set2", direction = 1) +
  ylab("proportion")
```

- There are more smokers of all types in 'stroke-yes' 

### Kids and Smoking

```{r}
df %>% filter(work_type == "children") %>% 
  group_by(smoking_status) %>% 
  summarise(N = n(), 
            avg.age = mean(age), 
            max.age = max(age), 
            min.age = min(age))
```

A lot of NAs in `smoking_status` comes from group 'Children' (see `work_type`). I can replace them with 'never smoked' during imputation stage of the analysis.

### Conclusions

There are several suspicious outliers (like in BMI and glucose). I can safely remove BMI > 75, maybe even BMI > 60 (Remember that  BMI > 40 is the highest class of obesity).

What is less safe - it's removing non-adults (younger than 20 y.o.). On one hand they provide valid information (age is very important predictor), on the other hand their stroke cases are really sus and a lot of predictors do not have sense (or are obvious NAs) for non-adults (like smoking, marriage status, employment type, residence type etc.); model-based imputation of `smoking_status` in children doesn't have sense as well, I should rather replace with "never smoked".

Since, modelling using all predictors and observations has given me very moderate results (TPR = 1 comes with very high FPR and very low probability cutoff close to 0), I will try various trimming of the data.

# Data preprocessing

## Stratified split

```{r}
set.seed(124)

data_split <- initial_split(df, prop = 3/4, strata = stroke)

df_train <- training(data_split)
df_test <- testing(data_split)
```

## 10-fold CV repeated 10 times

```{r}
set.seed(345)
# Stratified, repeated 10-fold cross-validation
cv_folds <- vfold_cv(df_train, strata = "stroke", v = 10, repeats = 10)

# metrics
cls_metrics <- metric_set(roc_auc, j_index)
```


## Recipe

```{r}
prep_recipe <- recipe(stroke ~ ., data = df_train) %>%
  step_impute_bag(bmi, smoking_status) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  #step_impute_bag(all_predictors()) %>% 
  step_normalize(age, avg_glucose_level, bmi) %>% 
  step_smote(stroke, over_ratio = 1, seed = 100) %>%
  check_missing(all_predictors()) %>% 
  step_zv(all_predictors())

prep_recipe
```

```{r, include=FALSE, eval=FALSE}
# the other way is to apply the recipe to your data immediately
# prep & bake
train_data <- prep_recipe %>% 
  prep(training = df_train) %>% 
  bake(new_data = NULL) # df_train will be processed

# bake test. what about SMOTE?
test_data <- prep_recipe %>% 
  prep(training = df_test) %>% 
  bake(new_data = df_test)

# check oversampling results
train_data %>% count(stroke) # SMOTE was applied
test_data %>% count(stroke) # not applied
```

# Penalized Logistic Regression

I add one more step to the recipe - remove correlated predictors (threshold = 0.75)

```{r}
# recipe for LR
lr_recipe <- prep_recipe %>% 
  step_corr(all_predictors(), threshold = 0.75)

# set model type/engine
lr_mod <- logistic_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet")

# define the workflow
lr_workflow <- 
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(lr_recipe)

# create a tune grid
lr_reg_grid <- tibble(penalty = 10**seq(-4, 0, length.out = 30))

# train and tune the model
lr_res <- tune_grid(lr_workflow,
              grid = lr_reg_grid,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE),
              metrics = cls_metrics)

autoplot(lr_res)
```

> The lower the penalty, the smaller the number of predictors used by the model. Such models should be preferred.

## Choose the best model

Here you see top 5 best models based on mean AUC and ranked by penalty score

```{r}
top_models <-
  lr_res %>% 
  show_best("roc_auc", n = 5) %>% 
  arrange(penalty) 

top_models %>% arrange(penalty)
```

I will choose a model with the highest mean AUC

```{r}
lr_best <- lr_res %>% 
  select_best(metric = "roc_auc")

lr_best
```

## ROC-AUC of the best model

```{r}
lr_auc <- 
  lr_res %>% 
  collect_predictions(parameters = lr_best) %>% 
  roc_curve(stroke, .pred_yes) %>% 
  mutate(model = "Logistic Regression")

autoplot(lr_auc)

```



# Random Forest

## Tuning

```{r}
set.seed(5732)

# number of cores available on Kaggle
cores <- 4L 

# model specification
rf_mod <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode("classification")

# join model and processing recipe
rf_cv_wf <- workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(prep_recipe)

# Stratified, repeated 10-fold cross-validation
cv_folds <- vfold_cv(df_train, strata = "stroke", v = 10, repeats = 5)

# metrics
cls_metrics <- metric_set(roc_auc, j_index)

# tune models, this takes time
rf_res <- tune_grid(rf_cv_wf,
            grid = 25,
            resamples = cv_folds,
            control = control_grid(save_pred = TRUE),
            metrics = cls_metrics)

autoplot(rf_res)
```

## Choose the best model

5 best models ranked by mean AUC

```{r}
rf_res %>% 
  show_best(metric = "roc_auc")
```

5 best models ranked by J-index

```{r}
rf_res %>% 
  show_best(metric = "j_index")
```

The best hyper-parameters look like this:

```{r}
rf_best <- rf_res %>% 
  select_best(metric = "roc_auc")

rf_best
```

## ROC-AUC of the best model

```{r}
rf_auc <- rf_res %>% 
  collect_predictions(parameters = rf_best) %>% 
  roc_curve(stroke, .pred_yes) %>% 
  mutate(model = "Random Forest")

autoplot(rf_auc) # 0.8
```

---

## Trimming

Remove BMI > 60

```{r trim, cache=F}
df_trim <- df %>% filter(bmi <= 60 | is.na(bmi))

skim(df_trim) %>% yank("factor")
```

4 cases with NA in `smoking_status` are gone.

```{r}
skim(df_trim) %>% yank("numeric")
```

## Imputation

### Manual

I will replace all NAs in Children (`work_type`) by 'never smoked'.

```{r}
df_trim$smoking_status <- ifelse((df_trim$work_type == "children" & is.na(df_trim$smoking_status)), 
                            "never smoked", as.character(df_trim$smoking_status))

df_trim$smoking_status <- as.factor(df_trim$smoking_status)
df_trim$smoking_status <- fct_relevel(df_trim$smoking_status, c("never smoked", "formerly smoked", "smokes"))

df_trim %>% filter(work_type == "children") %>% 
  group_by(smoking_status) %>% 
  summarise(N = n(), 
            avg.age = mean(age), 
            max.age = max(age), 
            min.age = min(age))
```

No NAs in children's smoking status anymore.

### Model-based imputation

The rest will be imputed using package [mice](https://www.rdocumentation.org/packages/mice/versions/3.14.0/topics/mice) and two models:

1. Proportional odds model (`polr`) for `smoking_status`

2. Predictive mean matching (`pmm`) for `bmi`


```{r, eval = T, message=FALSE, cache=F}
library(mice)

imp_mice <- mice(df_trim)
df_imp <- complete(imp_mice)

```

Number of NAs in `bmi`: `r sum(is.na(df_imp$bmi))`

Number of NAs in `smoking_status`: `r sum(is.na(df_imp$smoking_status))`

### Check distributions

I assume that distributions after imputation should not change significantly, i.e. distribution's shape and mean should remain the same.

#### BMI

There was no BMI imputation after trimming BMI > 60

```{r bmi.imp.plot, eval=F}
bmi_imp_comp <- bind_rows(select(df_trim, bmi, stroke) %>% mutate(type = rep("original", nrow(df_trim))),
          select(df_imp, bmi, stroke) %>% mutate(type = rep("imputed", nrow(df_imp))))

ggplot(bmi_imp_comp, aes(bmi)) +
  geom_histogram(aes(fill = type), alpha = 0.8) +
  facet_grid(cols = vars(stroke))
```

- Means and shape have not changed, which is good

#### Smoking status

```{r}
smoke_imp_comp <- bind_rows(select(df_trim, smoking_status, stroke) %>% mutate(type = rep("original", nrow(df_trim))),
          select(df_imp, smoking_status, stroke) %>% mutate(type = rep("imputed", nrow(df_imp))))

ggplot(smoke_imp_comp, aes(smoking_status)) +
  geom_bar(aes(fill=type), alpha=0.8, position="dodge") +
  facet_grid(cols = vars(stroke)) +
  xlab("")+
  theme(axis.text.x = element_text(angle=45, vjust = 0.5))
```

Counts increased proportionally in all `smoking_status` groups


## Scaling & Centering

Scale and center all the numeric features (including imputed BMI)

```{r scale}
# use caret::preProcess()
# preProcValues <- preProcess(training, method = c("center", "scale"))

df_scaled <- df_imp %>% 
  select(avg_glucose_level, age, bmi) %>% 
  scale() %>% 
  data.frame()

```


## Making dummy variables

Some variables (`hypertension`, `heart_disease`, `ever_married`) already have a *dummy* form.

The rest (`gender`, `work_type`, `Residence_type`, `smoking_status`) has to be *dummified*

```{r dummy}
# select vars
to_dum <- df_imp %>% select(gender, work_type, Residence_type, smoking_status)

# make an obj
dummies <- dummyVars(~ ., data = to_dum)

# apply it
df_dummy <- data.frame(predict(dummies, newdata = to_dum))

# look
head(df_dummy)
```

## Joining processed data

Join the scaled vars, the new dummies and the rest together.

```{r join}
df_proc <- bind_cols(df_scaled, df_dummy, select(df_trim, hypertension, heart_disease, ever_married, stroke))
head(df_proc)
```

Now we have 19 variables/features.

# Modelling

## Basic training parameters

5-fold cross validation repeated 10 times will be used.

ROC-optimization will be used (it is better when data is imbalanced).

```{r tune1}
# for ROC
fit_ctrl_roc <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 10, 
                           allowParallel = T,
                           classProbs = T,
                           summaryFunction = twoClassSummary)
```

## Split data

The data set is heavily imbalanced, so I will use **S**ynthetic **M**inority **O**versampling **Te**chnique (SMOTE) to create training data set, but not testing one.

```{r split}
set.seed(1234)
sample_set <- createDataPartition(y = df_proc$stroke, p = .75, list = FALSE)
df_train <- df_proc[sample_set,]
df_test <- df_proc[-sample_set,]

# DMwR::SMOTE for imbalanced data: over=225 and under=150 give me 1:1 ratio
df_train_smote <- SMOTE(stroke ~ ., data.frame(df_train), perc.over = 1725, perc.under = 106)

df_train_smote %>% group_by(stroke) %>% summarise(N=n())
```


Now the data set is balanced.

# Random Forest

## Training and validation

```{r train.rf}
set.seed(122)

#THREADS <- 6

#library(doParallel)

#cl <- makePSOCKcluster(THREADS)
#registerDoParallel(cl)

fit_rf <- train(stroke ~ ., 
                 data = df_train_smote, 
                 metric = "ROC", 
                 method = "rf", 
                 trControl = fit_ctrl_roc,
                 tuneGrid = expand.grid(.mtry = c(7.5, 8.5, 9.0, 9.5)),
                 verbosity = 0,
                 ntree = 25,
                 nodesize = 1,
                 verbose = FALSE)

#stopCluster(cl)

fit_rf

```


## Features importance

```{r featimp.rf}
imp_vars_rf <- varImp(fit_rf)

plot(imp_vars_rf, main = "Variable Importance with RF")

```


## Testing

Here's a function to extract data for ROC related plots below.

```{r roc.fun}
get_roc <- function(fit.obj, testing.df){
  pred_prob <- predict.train(fit.obj, newdata = testing.df, type = "prob")
  pred_roc <- prediction(predictions = pred_prob$yes, labels = testing.df$stroke)
  perf_roc <- performance(pred_roc, measure = "tpr", x.measure = "fpr")
  return(list(perf_roc, pred_roc))
}
```


### ROC-curve

```{r roc.rf}
# calculate ROC
perf_pred <- get_roc(fit_rf, df_test)
perf_rf <- perf_pred[[1]]
pred_rf <- perf_pred[[2]]

# take AUC 
auc_rf <- round(unlist(slot(performance(pred_rf, measure = "auc"), "y.values")), 3)

# plot
plot(perf_rf, main = "Random Fores ROC curve", col = "steelblue", lwd = 3)
abline(a = 0, b = 1, lwd = 3, lty = 2, col = 1)
legend(x = 0.7, y = 0.3, legend = paste0("AUC = ", auc_rf))

```


Ideally, we should be able to predict all stroke cases (TPR = 1.0). 

TPR/FPR cutoff of the model should be adjusted to 'catch' *all* stroke cases.

### TPR, FPR vs Probability cutoff

At which probability cutoff, you'll get TPR = 1.0?

```{r tpr.r}
# use pred_rf (pred_roc) object
plot(performance(pred_rf, measure = "tpr", x.measure = "cutoff"),
     col = "steelblue", 
     ylab = "Rate", 
     xlab = "Probability cutoff")

plot(performance(pred_rf, measure = "fpr", x.measure = "cutoff"), 
     add = T, col = "red")

legend(x = 0.65,y = 0.7, c("TPR (Recall)", "FPR (1-Spec)"), 
       lty = 1, col = c('steelblue', 'red'), bty = 'n', cex = 1, lwd = 2)

#abline(v = 0.02, lwd = 2, lty=6)

title("RF")
```

TPR curve is falling too quickly, while FPR is not falling quickly enough. 

The ideal cutoff should maximize TPR (i.e. Sensitivity/Recall) and minimize FPR.

In this case the ideal cutoff will be extremely low, invalidating the model.

### Confusion matrix

```{r test.rf}
# predict probabilities
pred_prob_rf <- predict(fit_rf, newdata = df_test, type = "prob")

# choose your cut-off
cutoff <- 0.01

# turn probabilities into classes
pred_class_rf <- ifelse(pred_prob_rf$yes > cutoff, "yes", "no")

pred_class_rf <- as.factor(pred_class_rf)

cm_rf <- confusionMatrix(data = pred_class_rf, 
                reference = df_test$stroke,
                mode = "everything",
                positive = "yes")

cm_rf
```

Chosen cutoff is `r cutoff`.

With this cutoff we've got very low Accuracy/Kappa, Specificity and PPV.

Extremely low PPV means that if the model predicts a patient to have a stroke, the chance that this patient *really* has a stroke is ~0.09

Is this model useful then? Not much. Although, Specificity and NPV are high, which means that if the model predicts 'no stroke', then with probability 0.99 there is indeed no stroke.

# Extreme Gradient Boosting: xgbTree

This *xgbTree* rendition has 7 parameters.

I've done standard hyper-parameters grid search for this model before (with `tuneLength=4`).

In the code below an optimal set of hyper-parameters is chosen.

## Training and validation

```{r train.xgb, eval=T}
set.seed(121)

# the best tune achieved by using tuneLength=4 is as the following:
# n_rounds = 50
# max_depth = 4
# eta = 0.3
# gamma = 0
# colsample_bytree = 0.6
# min_child_weight = 1
# subsample = 1

fit_xgb <- train(stroke ~ ., 
                 data = df_train_smote, 
                 method = "xgbTree",
                 metric = "ROC", 
                 trControl = fit_ctrl_roc,
                 tuneGrid = expand.grid(.nrounds = 50, 
                             .max_depth = 4,
                             .eta = 0.25,
                             .gamma = 0.01,
                             .colsample_bytree = 0.6,
                             .min_child_weight = 1,
                             .subsample = 1),
                 nthreads = 16,
                 verbose = FALSE,
                 verbosity = 0)

fit_xgb$bestTune
```


## Features importance

```{r featimp.xgb, eval=T}
imp_vars_xgb <- varImp(fit_xgb)

plot(imp_vars_xgb, main = "Variable Importance with XGB")
```


## Testing

### ROC curve

```{r roc.xgb, eval=T}
# calculate ROC
perf_pred_xgb <- get_roc(fit_xgb, df_test)
perf_xgb <- perf_pred_xgb[[1]]
pred_xgb <- perf_pred_xgb[[2]]


# take AUC 
auc_xgb <- round(unlist(slot(performance(pred_xgb, measure = "auc"), "y.values")), 3)

# plot
plot(perf_xgb, main = "xgbTree ROC curve", col = "steelblue", lwd = 3)
abline(a = 0, b = 1, lwd = 3, lty = 2, col = 1)
legend(x = 0.7, y = 0.3, legend = paste0("AUC = ", auc_xgb))

```

This ROC-curve looks much better than the one from Random Forest. It has a section with TPR=1.0 and FPR < 0.5.

### TPR v FPR

```{r tpr.xgb, eval=T}
# use pred_xgb object
plot(performance(pred_xgb, measure = "tpr", x.measure = "cutoff"),
     col = "steelblue", 
     ylab = "Rate", 
     xlab = "Probability cutoff")

plot(performance(pred_xgb, measure = "fpr", x.measure = "cutoff"), 
     add = T, col = "red")

legend(x = 0.6,y = 0.7, c("TPR (Recall)", "FPR (1-Spec)"), 
       lty = 1, col = c('steelblue', 'red'), bty = 'n', cex = 1, lwd = 2)

#abline(v = 0.1, lwd = 2, lty=6)

title("xgbTree")
```

The optimal cutoff here is till very low, with high Sensitivity it will bring low Specificity and PPV.

### Confusion matrix

```{r cm.xgb, eval=T}
pred_prob_xgb <- predict(fit_xgb, newdata=df_test, type = "prob")

# choose your cut-off
cutoff <- 0.025

# turn probabilities into classes
pred_class_xgb <- ifelse(pred_prob_xgb$yes > cutoff, "yes", "no")

pred_class_xgb <- as.factor(pred_class_xgb)

cm_xgb <- confusionMatrix(data = pred_class_xgb, 
                reference = df_test$stroke,
                mode = "everything",
                positive = "yes")

cm_xgb
```

Chosen cutoff is `r cutoff`.

This model performs better then the previous one but not well enough - we still have a lot of false positive results.

My guess is that life style is a poor predictor of stroke. In order to get better performing models, we need more data related to physiology, biochemistry or genetics of patients.

# Save the workspace

---
```{r save, eval=T}
# save it
save.image("data/workspace.RData")
```

