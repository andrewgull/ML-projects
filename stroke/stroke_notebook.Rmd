---
title: "Stroke prediction"
author: "AG"
date: "last update: `r format(Sys.Date(), format = '%d %B %Y')`"
output:
  prettydoc::html_pretty:
    toc: yes
    df_print: paged
    fig_width: 10
    fig_height: 6
    theme: cayman
    highlight: vignette
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F, cache = F)
library(tidyverse)
library(caret)
#library(GGally)
library(DMwR) # installed via archive: install.packages("/Path/to/DMwR_0.4.1.tar.gz", repos=NULL, type="source" )
library(ROCR)

THREADS <- 10
```

## Read data

In `smoking_status` Unknown should be changed to NA.

Also, it can be ordered: never < formerly < smokes

Other predictors seem to be OK

```{r read}
df <- read_csv("data/healthcare-dataset-stroke-data.csv", col_types = "cfdfffffddcf", na = c("Unknown", "N/A"))
# if you set smoking_status to factor in col_types, na() won't work
df$smoking_status <- as_factor(df$smoking_status)
df$smoking_status <- fct_relevel(df$smoking_status, c("never smoked", "formerly smoked", "smokes"))
df$stroke <- factor(ifelse(df$stroke == 1, "yes", "no"), levels = c("no", "yes"))

df
```

## skimr description

Skip id column

```{r describe}
df$id <- NULL
skimr::skim_to_wide(df)
```

Target 'stroke' is imbalanced!

Smoking's complete rate 0.7

#### How many snoking_status in each target class?

```{r}
df %>% group_by(stroke, smoking_status) %>% summarise(N=n())
```


BMI's complete rate 0.96

#### How many skipped BMI in each target class?

```{r}
df %>% filter(is.na(bmi)) %>% group_by(stroke) %>% summarise(N=n())
```


One 'Other' gender to be removed

```{r filter}
df <- df %>% filter(gender != "Other")
```


## EDA

### Overview: a pairs plot

```{r pairs, fig.width=10, fig.height=10, warning=FALSE, message=FALSE, eval=FALSE}
GGally::ggpairs(df, aes(color = stroke, alpha = 0.2, dotsize = 0.02), 
        upper = list(continuous = GGally::wrap("cor", size = 2.5)),
        diag = list(continuous = "barDiag")) +
  scale_color_brewer(palette = "Set1", direction = -1) +
  scale_fill_brewer(palette = "Set1", direction = -1)
```

### In details

#### Stroke vs Age

```{r}
ggplot(df, aes(stroke, age)) +
  geom_boxplot(aes(fill = stroke), alpha = 0.5, varwidth = T, notch = T) +
  geom_violin(aes(fill = stroke), alpha = 0.5) +
  scale_fill_brewer(palette = "Set1", direction = -1) +
  xlab("")
```

#### Stroke vs Age + Gender

```{r box1}
ggplot(df, aes(stroke, age)) + 
  geom_violin(alpha=0.3) +
  geom_jitter(alpha=0.2, size=0.8, width = 0.15, height = 0.1, aes(color = gender)) + 
  geom_boxplot(alpha = 0.2) +
  scale_color_brewer(palette = "Set2", direction = -1)
```

On average, age is higher among stroke-yes group, there is no interaction between age and gender

#### Stroke vs BMI

```{r}
ggplot(df, aes(stroke, bmi)) +
  geom_boxplot(aes(fill = stroke), alpha = 0.5, varwidth = T, notch = T) +
  geom_violin(aes(fill = stroke), alpha = 0.5) +
  scale_fill_brewer(palette = "Set1", direction = -1) +
  xlab("")
```


#### Stroke vs Gender

```{r bar1}
gender <- df %>% group_by(stroke, gender) %>% summarize(N=n())

ggplot(gender, aes(stroke, N)) +
  geom_bar(aes(fill=gender), alpha = 0.8, stat = "identity", position = "fill")+
  scale_fill_brewer(palette = "Set2", direction = -1)+
  ylab("proportion")
```

Proportions in both stroke groups are roughly the same

#### Stroke vs Hypertension

```{r bar2}
hyptens <- df %>% group_by(stroke, hypertension) %>% summarize(N=n())

hyptens

ggplot(hyptens, aes(stroke, N)) +
  geom_bar(aes(fill=hypertension), alpha = 0.8, stat = "identity", position = "fill")+
  scale_fill_brewer(palette = "Set2", direction = -1)+
  ylab("proportion")
```
Hypertension occurred more often among stroke-yes

```{r}
ggplot(df, aes(stroke, avg_glucose_level)) +
  geom_boxplot(aes(fill = stroke), alpha = 0.5, varwidth = T, notch = T) +
  geom_violin(aes(fill = stroke), alpha = 0.5) +
  scale_fill_brewer(palette = "Set1", direction = -1) +
  xlab("")
```

## Imputation

Using package [mice](https://www.rdocumentation.org/packages/mice/versions/3.14.0/topics/mice)

It uses `polr` - proportional odds model - for `smoking_status` and `pmm` - predictive mean matching - for `bmi`

### Run imputation

```{r, eval = T, message=FALSE}
library(mice)

imp_mice <- mice(df)
df_imp <- complete(imp_mice)

```

Number of NAs in BMI: `r sum(is.na(df_imp$bmi))`

Number of NAs in Smoking `r sum(is.na(df_imp$smoking_status))`

### Check distributions

#### BMI

```{r}
bmi_imp_comp <- bind_rows(select(df, bmi, stroke) %>% mutate(type = rep("original", nrow(df))),
          select(df_imp, bmi, stroke) %>% mutate(type = rep("imputed", nrow(df_imp))))

ggplot(bmi_imp_comp, aes(bmi)) +
  geom_histogram(aes(fill=type), alpha=0.8) +
  facet_grid(cols = vars(stroke))
```

Means have not changed, which is good, I suppose.

#### Smoking

```{r}
smoke_imp_comp <- bind_rows(select(df, smoking_status, stroke) %>% mutate(type = rep("original", nrow(df))),
          select(df_imp, smoking_status, stroke) %>% mutate(type = rep("imputed", nrow(df_imp))))

ggplot(smoke_imp_comp, aes(smoking_status)) +
  geom_bar(aes(fill=type), alpha=0.8, position="dodge") +
  facet_grid(cols = vars(stroke)) +
  xlab("")+
  theme(axis.text.x = element_text(angle=45, vjust = 0.5))
```

Counts increased proportionally in all *Smoking* groups


## Scaling & Normalization

Scale numeric features (including imputed BMI)

```{r scale}
# use caret::preProcess()
# preProcValues <- preProcess(training, method = c("center", "scale"))

df_scaled <- df_imp %>% 
  select(avg_glucose_level, age, bmi) %>% 
  scale() %>% 
  data.frame()

```


## Make Dummies

I've decided to omit smoking_status completely - it won't be *dummified*

```{r dummy}
# select vars
to_dum <- df_imp %>% select(gender, ever_married, work_type, Residence_type, smoking_status)
# make an obj
dummies <- dummyVars(~ ., data=to_dum)
# apply it
df_dummy <- data.frame(predict(dummies, newdata=to_dum))

head(df_dummy)
```

## Join scaled and dummies and the rest

```{r join}
df_proc <- bind_cols(df_scaled, df_dummy, select(df, hypertension, heart_disease, stroke))
head(df_proc)
```

# Modelling

## Params tuning

ROC-optimization is better when data is imbalanced

Kappa-optimization is also good

```{r tune}
# for ROC
fit_ctrl_roc <- trainControl(## 5-fold CV
                           method = "repeatedcv",
                           number = 5,
                           repeats = 10, 
                           allowParallel = T,
                           classProbs = T,
                           summaryFunction = twoClassSummary)
# for kappa
fit_ctrl_kp <- trainControl(## 5-fold CV
                           method = "repeatedcv",
                           number = 5,
                           repeats = 10, 
                           allowParallel = T)

fit_ctrl_kp10 <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           repeats = 10, 
                           allowParallel = T)
```

## Split data

Imbalanced data - use SMOTE to create training data set, but not testing data set

```{r split}
set.seed(1234)
sample_set <- createDataPartition(y = df_proc$stroke, p = .75, list = FALSE)
df_train <- df_proc[sample_set,]
df_test <- df_proc[-sample_set,]

# DMwR::SMOTE for imbalanced data: over=225 and under=150 give me 1:1 ratio
df_train_smote <- SMOTE(stroke ~ ., data.frame(df_train), perc.over = 1725, perc.under = 106)

df_train_smote %>% group_by(stroke) %>% summarise(N=n())
```


# RF

## Training and validation

### Kappa-optimized

For imbalanced classes

```{r train.rf, eval=T}
library(doParallel)

cl <- makePSOCKcluster(THREADS)
registerDoParallel(cl)

set.seed(123)

fit_rf <- train(stroke ~ ., 
                 data = df_train_smote, 
                 metric = "Kappa", 
                 method = "rf", 
                 trControl = fit_ctrl_kp,
                 tuneGrid = expand.grid(.mtry = seq(2, 6, 0.5)), # I've tried all values greater than these
                 verbosity = 0,
                 verbose = FALSE)
stopCluster(cl)

fit_rf

```

### ROC-optimized

```{r train.rf.roc, eval=T}
cl <- makePSOCKcluster(THREADS)
registerDoParallel(cl)

set.seed(120)

fit_rf_roc <- train(stroke ~ ., 
                 data = df_train_smote, 
                 metric = "ROC", 
                 method = "rf", 
                 trControl = fit_ctrl_roc,
                 tuneGrid = expand.grid(.mtry = seq(2, 6, 0.5)),
                 verbosity = 0,
                 verbose = FALSE)
stopCluster(cl)

fit_rf_roc

```

## Features importance

### Kappa-optimized model

```{r featimp.rf, eval=T}
imp_vars_rf <- varImp(fit_rf)

plot(imp_vars_rf, main = "Variable Importance with RF")

```

### ROC-optimized model

it's the same

## Testing

### ROC & AUC

a function for roc-stuff

```{r roc.fun}
get_roc <- function(fit.obj, testing.df){
  pred_prob <- predict.train(fit.obj, newdata = testing.df, type = "prob")
  pred_roc <- prediction(predictions = pred_prob$yes, labels = testing.df$stroke)
  perf_roc <- performance(pred_roc, measure = "tpr", x.measure = "fpr")
  return(list(perf_roc, pred_roc))
}
```

#### ROC-curve for kappa-optimized model

```{r roc.rf, eval=T}
# calculate ROC
perf_pred <- get_roc(fit_rf, df_test)
perf_rf <- perf_pred[[1]]
pred_rf <- perf_pred[[2]]

# take AUC 
auc_rf <- round(unlist(slot(performance(pred_rf, measure = "auc"), "y.values")), 3)

# plot
plot(perf_rf, main = "RF-k ROC curve", col = "steelblue", lwd = 3)
abline(a = 0, b = 1, lwd = 3, lty = 2, col = 1)
legend(x = 0.7, y = 0.3, legend = paste0("AUC = ", auc_rf))

```

#### ROC-curve for ROC-optimized model

```{r roc.rf.roc, eval=T}
# calculate ROC
perf_pred_roc <- get_roc(fit_rf_roc, df_test)
perf_rf_roc <- perf_pred_roc[[1]]
pred_rf_roc <- perf_pred_roc[[2]]

# take AUC 
auc_rf_roc <- round(unlist(slot(performance(pred_rf_roc, measure = "auc"), "y.values")), 3)

# plot
plot(perf_rf_roc, main = "RF-r ROC curve", col = "steelblue", lwd = 3)
abline(a = 0, b = 1, lwd = 3, lty = 2, col = 1)
legend(x = 0.7, y = 0.3, legend = paste0("AUC = ", auc_rf_roc))

```

So, we can adjust TPR/FPR cutoff to predict all strokes

### TPR, FPR vs Probability cutoff

At which probability cut-off, you'll get TPR = 1.0?

```{r tpr.k}
# use pred_rf (pred_roc) object
plot(performance(pred_rf, measure = "tpr", x.measure = "cutoff"),
     col="steelblue", 
     ylab = "Rate", 
     xlab="Probability cutoff")

plot(performance(pred_rf, measure = "fpr", x.measure = "cutoff"), 
     add = T, col = "red")

legend(x = 0.6,y = 0.7, c("TPR (Recall)", "FPR (1-Spec)"), 
       lty = 1, col =c('steelblue', 'red'), bty = 'n', cex = 1, lwd = 2)

abline(v = 0.02, lwd = 2, lty=6)

title("RF-k")
```

Vertical line at cutoff = 0.02 designates maximum TPR and maximum FPR. Ideal cutoff should be to the left of this line

```{r tpr.r}
# use pred_rf (pred_roc) object
plot(performance(pred_rf_roc, measure = "tpr", x.measure = "cutoff"),
     col = "steelblue", 
     ylab = "Rate", 
     xlab = "Probability cutoff")

plot(performance(pred_rf_roc, measure = "fpr", x.measure = "cutoff"), 
     add = T, col = "red")

legend(x = 0.6,y = 0.7, c("TPR (Recall)", "FPR (1-Spec)"), 
       lty = 1, col = c('steelblue', 'red'), bty = 'n', cex = 1, lwd = 2)

abline(v = 0.02, lwd = 2, lty=6)

title("RF-r")
```

Vertical line at 0.02

### Confusion matrix

#### Kappa-optimized

Using desired cut-off: we want to maximize TPR (Sensitivity, Recall)!

According to the TPR/FPR plot (above) the optimal cutoff is

```{r test.rf.k}
# predict probabilities
pred_prob_rf <- predict(fit_rf, newdata=df_test, type = "prob")

# choose your cut-off
cutoff = 0.01

# turn probabilities into classes
pred_class_rf <- ifelse(pred_prob_rf$yes > cutoff, "yes", "no")

pred_class_rf <- as.factor(pred_class_rf)

cm_rf <- confusionMatrix(data = pred_class_rf, 
                reference = df_test$stroke,
                mode = "everything",
                positive = "yes")

cm_rf
```

#### ROC-optimized

```{r test.rf.r}
# predict probabilities
pred_prob_rf_roc <- predict(fit_rf_roc, newdata = df_test, type = "prob")

# choose your cut-off
cutoff = 0.01

# turn probabilities into classes
pred_class_rf_roc <- ifelse(pred_prob_rf_roc$yes > cutoff, "yes", "no")

pred_class_rf_roc <- as.factor(pred_class_rf_roc)

cm_rf <- confusionMatrix(data = pred_class_rf_roc, 
                reference = df_test$stroke,
                mode = "everything",
                positive = "yes")

cm_rf
```


# AdaBoost

## Training and validation

### Kappa-optimized

For imbalanced classes

```{r train.lr, eval=T}
set.seed(122)

cl <- makePSOCKcluster(THREADS)
registerDoParallel(cl)

fit_adb <- train(stroke ~ ., 
                 data = df_train_smote, 
                 metric = "Kappa", 
                 method = "AdaBoost.M1", 
                 trControl = fit_ctrl_kp10,
                 tuneGrid = expand.grid(.maxdepth = seq(10, 20, 2), .mfinal = seq(150, 180, 10), .coeflearn = c("Freund")),
                 verbosity = 0,
                 verbose = FALSE)

# coeflearn=Freund was chosen by automatic grid search, mfinal choice comes from there too

stopCluster(cl)

fit_adb

```

## Testing

### ROC curve

```{r}
# calculate ROC
perf_pred_adb <- get_roc(fit_adb, df_test)
perf_adb <- perf_pred_adb[[1]]
pred_adb <- perf_pred_adb[[2]]

# take AUC 
auc_adb <- round(unlist(slot(performance(pred_adb, measure = "auc"), "y.values")), 3)

# plot
plot(perf_adb, main = "AdaBoost ROC curve", col = "steelblue", lwd = 3)
abline(a = 0, b = 1, lwd = 3, lty = 2, col = 1)
legend(x = 0.7, y = 0.3, legend = paste0("AUC = ", auc_adb))
```

### TPR, FPR vs Probability cutoff

At which probability cut-off, you'll get TPR = 1.0?

```{r}
# use pred_rf (pred_roc) object
plot(performance(pred_adb, measure = "tpr", x.measure = "cutoff"),
     col="steelblue", 
     ylab = "Rate", 
     xlab="Probability cutoff")

plot(performance(pred_adb, measure = "fpr", x.measure = "cutoff"), 
     add = T, col = "red")

legend(x = 0.6,y = 0.7, c("TPR (Recall)", "FPR (1-Spec)"), 
       lty = 1, col =c('steelblue', 'red'), bty = 'n', cex = 1, lwd = 2)

abline(v = 0.1, lwd = 2, lty=6)

title("AdaBoost.M1")
```

### Confusion matrix

```{r}
pred_prob_adb <- predict(fit_adb, newdata=df_test, type = "prob")

# choose your cut-off
cutoff = 0.16

# turn probabilities into classes
pred_class_adb <- ifelse(pred_prob_adb$yes > cutoff, "yes", "no")

pred_class_adb <- as.factor(pred_class_adb)

cm_adb <- confusionMatrix(data = pred_class_adb, 
                reference = df_test$stroke,
                mode="everything",
                positive="yes")

cm_adb
```


# XGB

## Training and validation

### ROC optimized

```{r train.xgb, eval=T, cache=T, eval=F}
set.seed(123)

fit_xgb_roc <- train(stroke ~ ., 
                 data = df_train_smote, 
                 method = "xgbDART",
                 metric = "ROC", 
                 trControl = fit_ctrl_roc,
                 tuneLength = 2,
                 verbose = FALSE,
                 verbosity = 0)

fit_xgb_roc$bestTune
```


## Features importance

```{r featimp.xgb, eval=F}
imp_vars_xgb <- varImp(fit_xgb_roc)

plot(imp_vars_xgb, main = "Variable Importance with XGB")
```


## Testing

### ROC & AUC


```{r roc.xgb, eval=F}
# calculate ROC
perf_pred_xgb <- get_roc(fit_xgb_roc, df_test)
perf_roc_xgb <- perf_pred_xgb[[1]]
pred_roc_xgb <- perf_pred_xgb[[2]]


# take AUC 
auc_xgb <- round(unlist(slot(performance(pred_roc_xgb, measure = "auc"), "y.values")), 3)

# plot
plot(perf_roc_xgb, main = "XGB ROC curve", col = "steelblue", lwd = 3)
abline(a = 0, b = 1, lwd = 3, lty = 2, col = 1)
legend(x = 0.7, y = 0.3, legend = paste0("AUC = ", auc_xgb))

```

# Save the workspace

---
```{r save, eval=T}
save.image("data/workspace.RData")
```

