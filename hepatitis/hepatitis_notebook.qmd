---
title: "Hepatits C prediction using R and tidymodels"
author: "A.G."
date-modified: "last update: `r format(Sys.Date(), format = '%d %B %Y')`"
format: 
  html:
    toc: true
    toc-depth: 2
    toc-title: Contents
    toc-location: left
    df-print: paged
    standalone: true
    code-fold: true
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F, cache = T)
library(tidymodels)
library(skimr) # descriptive stats
library(stringr) # same idea
library(themis) # for SMOTE and other recipes for target balancing
library(vip) # for variable importance
library(probably) # for performance calibration
library(bestNormalize) # for ORQ-norm
library(ggforce) # for validation results
library(embed) # for UMAP
library(tidyposterior) # for model comparison
```

# Intro

The aim is to predict development of hepatitis' stages.

> The target attribute for classification is Category (2): blood donors vs. Hepatitis C patients (including its progress ('just' Hepatitis C, Fibrosis, Cirrhosis).

I'll join *Cirrhosis*, *Fibrosis* and *Hepatitis C* into one category **Hepatitis**, the other one will be called **Donor**

# Read data set

The data set looks like this:

```{r}
df <- readr::read_csv("data/HepatitisCdata.csv", col_types = "dfdfdddddddddd") %>% select(-...1)

# make re-coded Category variable
# there is 0=Blood Donor and 0s=suspect Blood Donor
df <- df %>% 
  mutate(Diagnosis = if_else(str_detect(Category, "Donor"), "Donor", "Hepatitis")) %>%
  mutate(Diagnosis = factor(Diagnosis, levels = c("Hepatitis", "Donor"))) %>%
  relocate(Diagnosis, .before = Category) %>%
  select(-Category)

head(df)
```

# Descriptive statistics

## Factor variables

```{r}
skim(df) %>%
  yank("factor")
```

No missing data here, but the target variable is imbalanced: 75 positive cases vs 540 negative cases

## Numeric variables

```{r}
skim(df) %>%
  yank("numeric")
```

Numeric variables are mostly complete. I will use imputation later to fill in the gaps

# EDA

## Functions

Some useful functions that I use throughout the notebook

```{r}
# Create the scatter plot matrix
plot_validation_results <- function(dat, outcome) {
  dat %>%
    ggplot(aes(x = .panel_x, y = .panel_y, color = {{outcome}}, fill = {{outcome}})) +
    geom_point(alpha = 0.4, size = 1) +
    geom_autodensity(alpha = .3) +
    facet_matrix(vars(-{{outcome}}), layer.diag = 2) + 
    scale_color_brewer(palette = "Set1") + 
    scale_fill_brewer(palette = "Set1")
}

# function to get a name of the best preprocessor from a resampling object
best_preprocessor <- function(res_obj) {
  res_obj %>% select_best("roc_auc") %>% pull(.config)
}

# function to make a df with AUCs from the best preprocessor
best_aucs <- function(res_obj, model_name){
  res_obj %>%
    collect_metrics(summarize = FALSE) %>%
    filter(.metric == "roc_auc", .config == best_preprocessor(res_obj)) %>%
    select(id, id2, {{model_name}} := .estimate)
}

## MK ROC OBJECT ##
# use with autoplot()
make_roc <- function(mod_res, title){
  
  mod_best <- mod_res %>%
    select_best(metric = "roc_auc")
  
  mod_auc <- mod_res %>% 
    collect_predictions(parameters = mod_best) %>% 
    roc_curve(Diagnosis, .pred_Hepatitis) %>% 
    mutate(model = title)
  
  return(mod_auc)
}

```

## Pairs plot

```{r pairs, fig.width=14, fig.height=10, warning=FALSE, message=FALSE}
GGally::ggpairs(df, aes(color = Diagnosis), 
        upper = list(continuous = GGally::wrap("cor", size = 3.0)),
        diag = list(continuous = "barDiag"),
        lower = list(continuous = GGally::wrap("points", alpha = 0.3, size=0.8))) +
  scale_color_brewer(palette = "Set1", direction = 1) +
  scale_fill_brewer(palette = "Set1", direction = 1)
```

## Correlation plot

using Spearman's correlation coefficient

```{r}
# without resistance
cor_matrix <- df %>%
  select(-c(
    Sex,
    Diagnosis
  )) %>%
  cor(use = "pairwise.complete.obs", method = "spearman")

corrplot::corrplot(cor_matrix, type = "upper", tl.col = "black", tl.cex = 0.6)
```

AST and GGT are correlated, Protein and Albumin are correlated as well, although these coefficients are quite low (approx. +0.5)

Most likely there is no need to use PCA.

## PCA

```{r}
pca_rec <- recipe(Diagnosis ~., data = df) %>%
  step_nzv(all_predictors()) %>% 
  step_impute_median(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_orderNorm(all_numeric_predictors()) %>% 
  step_pca(all_predictors())

pca_prep <- prep(pca_rec)

sdev <- pca_prep$steps[[5]]$res$sdev
percent_variation <- sdev^2 / sum(sdev^2)

var_df <- data.frame(PC = paste0("PC", 1:length(sdev)),
                     var_explained = percent_variation,
                     stringsAsFactors = FALSE)

var_df <- var_df %>% 
  mutate(var_cum_sum = cumsum(var_explained))
```

```{r}
var_df %>%
  mutate(PC = forcats::fct_inorder(PC)) %>%
  ggplot(aes(x = PC, y = var_explained)) + 
  geom_col(aes(fill = var_cum_sum)) +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5)) +
  scale_fill_continuous(type = "viridis")
```

Number of principal components is the same as the number of predictors.

```{r, fig.height=4, fig.width=12}
tidied_pca <- tidy(pca_prep, 5)

tidied_pca %>%
  filter(component %in% paste0("PC", 1:6)) %>%
  mutate(component = forcats::fct_inorder(component)) %>%
  ggplot(aes(value, terms, fill = terms)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~component, nrow = 1) +
  labs(y = NULL)
```

```{r, fig.width=10, fig.height=8}
pca_prep %>%
  juice() %>%
  plot_validation_results(outcome = Diagnosis) +
  ggtitle("PCA")
```

Linear classifier might do some job, but not perfect.

## UMAP

Supervised non-linear dimensinality reduction technique.

```{r, fig.width=10, fig.height=8}
umap_rec <- recipe(Diagnosis ~., data = df) %>%
  step_nzv(all_predictors()) %>% 
  step_impute_median(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_orderNorm(all_numeric_predictors()) %>% 
  step_umap(all_predictors(), 
            neighbors = 15, 
            min_dist = 0.01, 
            num_comp = 6, 
            outcome = "Diagnosis")

umap_results <- umap_rec %>% prep() %>% juice()

umap_results %>%
  plot_validation_results(outcome = Diagnosis) +
  ggtitle("Supervised UMAP")
```

Here we can see that majority of Hepatitis cases are separated from non-Hepatitis

### 3D plot

```{r}
library(plotly)

umap3d <- plot_ly(
  umap_results,
  x = ~ UMAP1,
  y = ~ UMAP2,
  z = ~ UMAP3,
  color = ~ umap_results$Diagnosis,
  colors = c('#cf280c', '#1b56f7')
) %>%
  add_markers(size = 2,
              text = ~ umap_results$Diagnosis) %>%
  layout(scene = list(
    xaxis = list(title = 'UMAP1'),
    yaxis = list(title = 'UMAP2'),
    zaxis = list(title = 'UMAP3')
  )) 

umap3d
```

## ICA

```{r, fig.width=10, fig.height=8}
ica_rec <- recipe(Diagnosis ~., data = df) %>%
  step_nzv(all_predictors()) %>% 
  step_impute_median(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_orderNorm(all_numeric_predictors()) %>% 
  step_ica(all_numeric_predictors(), num_comp = 8)

data_ica <- ica_rec %>% prep() %>% juice()

data_ica %>% 
  plot_validation_results(outcome = Diagnosis) + 
  ggtitle("ICA")
```

## Age v Diagnosis

```{r}
ggplot(df, aes(Diagnosis, Age)) +
  geom_violin(aes(fill = Diagnosis), alpha = 0.5) +
  geom_jitter(alpha = 0.5, size = 0.6) +
  geom_boxplot(aes(fill = Diagnosis), alpha = 0.2) +
  scale_fill_brewer(palette = "Set1", direction = 1) +
  xlab("")
```

## Age v ALB

```{r}
ggplot(df, aes(Diagnosis, ALB)) +
  geom_violin(aes(fill = Diagnosis), alpha = 0.5) +
  geom_jitter(alpha = 0.5, size = 0.6) +
  geom_boxplot(aes(fill = Diagnosis), alpha = 0.2) +
  scale_fill_brewer(palette = "Set1", direction = 1) +
  xlab("")
```

## ALP v Diagnosis

```{r}
ggplot(df, aes(Diagnosis, ALP)) +
  geom_violin(aes(fill = Diagnosis), alpha = 0.5) +
  geom_jitter(alpha = 0.5, size = 0.6) +
  geom_boxplot(aes(fill = Diagnosis), alpha = 0.2) +
  scale_fill_brewer(palette = "Set1", direction = 1) +
  xlab("")
```

## ALT v Diagnosis

```{r}
ggplot(df, aes(Diagnosis, ALT)) +
  geom_violin(aes(fill = Diagnosis), alpha = 0.5) +
  geom_jitter(alpha = 0.5, size = 0.6) +
  geom_boxplot(aes(fill = Diagnosis), alpha = 0.2) +
  scale_fill_brewer(palette = "Set1", direction = 1) +
  coord_trans(y = "sqrt") +
  xlab("")
```

## AST v Diagnosis

```{r}
ggplot(df, aes(Diagnosis, AST)) +
  geom_violin(aes(fill = Diagnosis), alpha = 0.5) +
  geom_jitter(alpha = 0.5, size = 0.6) +
  geom_boxplot(aes(fill = Diagnosis), alpha = 0.2) +
  scale_fill_brewer(palette = "Set1", direction = 1) +
  coord_trans(y = "sqrt") +
  xlab("")
```

## BIL v Diagnosis

```{r}
ggplot(df, aes(Diagnosis, BIL)) +
  geom_violin(aes(fill = Diagnosis), alpha = 0.5) +
  geom_jitter(alpha = 0.5, size = 0.6) +
  geom_boxplot(aes(fill = Diagnosis), alpha = 0.2) +
  scale_fill_brewer(palette = "Set1", direction = 1) +
  coord_trans(y = "sqrt") +
  xlab("")
```

## CHE v Diagnosis

```{r}
ggplot(df, aes(Diagnosis, CHE)) +
  geom_violin(aes(fill = Diagnosis), alpha = 0.5) +
  geom_jitter(alpha = 0.5, size = 0.6) +
  geom_boxplot(aes(fill = Diagnosis), alpha = 0.2) +
  scale_fill_brewer(palette = "Set1", direction = 1) +
  xlab("")
```

## CHOL v Diagnosis

```{r}
ggplot(df, aes(Diagnosis, CHOL)) +
  geom_violin(aes(fill = Diagnosis), alpha = 0.5) +
  geom_jitter(alpha = 0.5, size = 0.6) +
  geom_boxplot(aes(fill = Diagnosis), alpha = 0.2) +
  scale_fill_brewer(palette = "Set1", direction = 1) +
  xlab("")
```

## CREA v Diagnosis

```{r}
ggplot(df, aes(Diagnosis, CREA)) +
  geom_violin(aes(fill = Diagnosis), alpha = 0.5) +
  geom_jitter(alpha = 0.2, size = 0.3) +
  geom_boxplot(aes(fill = Diagnosis), alpha = 0.2) +
  scale_fill_brewer(palette = "Set1", direction = 1) +
  coord_trans(y = "sqrt") +
  xlab("")
```

## GGT v Diagnosis

```{r}
ggplot(df, aes(Diagnosis, GGT)) +
  geom_violin(aes(fill = Diagnosis), alpha = 0.5) +
  geom_jitter(alpha = 0.5, size = 0.6) +
  geom_boxplot(aes(fill = Diagnosis), alpha = 0.2) +
  scale_fill_brewer(palette = "Set1", direction = 1) +
  coord_trans(y = "sqrt") +
  xlab("")
```

Median of most numerical variables are different between `Donor` and `Hepatitis` groups.

## Sex v Diagnosis

```{r}
sex_n <- df %>% group_by(Diagnosis, Sex) %>% summarise(N = n())

ggplot(sex_n, aes(Diagnosis, N)) +
  geom_col(aes(fill = Sex), position = "fill") +
  scale_fill_brewer(palette = "Set2", direction = 1) +
  xlab("") +
  ylab("Proportion")
```

Difference in proportion of each sex is not that big between `Donor` and `Hepatitis` groups.

# Data processing

The following analysis is based on tidyverse tutorials and examples [here](https://www.tidymodels.org/start/recipes/), [here](https://www.tidymodels.org/start/resampling/) and [here](https://www.tidymodels.org/learn/work/tune-svm/).

## Split

Stratified splitting because target class is imbalanced.

```{r data.split}
set.seed(124)

data_split <- initial_split(df, prop = 3/4, strata = Diagnosis)

df_train <- training(data_split)
df_test <- testing(data_split)
```

## Folds and metrics

> If a model is poorly calibrated, the ROC curve value might not show diminished performance. However, the J index would be lower for models with pathological distributions for the class probabilities.

> Youden's J statistic is defined as: sens + spec - 1

> The value of the J-index ranges from \[0, 1\] and is 1 when there are no false positives and no false negatives.

Stratified, repeated 10-fold cross-validation will be used.

ROC and J-index will be used as maximization metrics.

```{r}
# Stratified, repeated 10-fold cross-validation is used to resample the model:
cv_folds <- vfold_cv(df_train, strata = "Diagnosis", v = 10, repeats = 10) 
# metrics for imbalanced classes
cls_metrics <- metric_set(roc_auc, j_index)
```

## Preprocessing

-   Impute using median

-   Turn nominal variables into dummies (it is not required by tree-based models or logistic regression, but without this step oversampling step will not work)

-   Normalize all predictors

-   Balance target classes using SMOTE algorithm

The preprocessing recipe looks like this:

```{r}
basic_recipe <- recipe(Diagnosis ~ ., data = df_train) %>%
  step_impute_median(all_numeric_predictors()) %>% # I use median bcz not that many observations are missing
  step_dummy(all_nominal_predictors()) %>% # dummy goes before normalisation
  step_normalize(all_predictors()) %>% 
  step_smote(Diagnosis, over_ratio = 1, seed = 100) %>% # original target distribution 399 v 62
  check_missing(all_predictors())

basic_recipe
```

```{r, include=FALSE, eval=FALSE}
# the other way is to apply the recipe to your data immediately
# prep & bake
train_data <- basic_recipe %>% 
  prep(training = df_train) %>% 
  bake(new_data = NULL) # df_train will be processed

# bake test. what about SMOTE?
test_data <- basic_recipe %>% 
  prep( training = df_test) %>% 
  bake(new_data = df_test)

# check oversampling results
train_data %>% count(Diagnosis) # SMOTE was applied
test_data %>% count(Diagnosis) # not applied
```

# Tune and fit penalized logistic regression

For regularized logistic regression, package [glmnet](https://glmnet.stanford.edu/articles/glmnet.html) will be used.

Recipe for this model is the same as for RF models + removing of correlated variables (correlation threshold = 0.75)

```{r}
# recipe for LR
lr_recipe <- recipe(Diagnosis ~ ., data = df_train) %>%
  step_impute_median(all_numeric_predictors()) %>% # I use median bcz not that many observations are missing
  step_dummy(all_nominal_predictors()) %>% # dummy goes before normalisation
  step_normalize(all_predictors()) %>% 
  step_corr(threshold = tune("corr_tune")) %>%
  step_smote(Diagnosis, over_ratio = 1, seed = 100)

# set model type/engine
lr_mod <- logistic_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet", num.threads = 4)

# define the workflow
lr_workflow <- 
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(lr_recipe)

# create a tune grid
# lr_reg_grid <- tibble(penalty = 10**seq(-4, 0, length.out = 30))
# instead of predefined tune grid, I will use default space-filling grid
# train and tune the model
lr_res <- tune_grid(lr_workflow,
              grid = 30,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE),
              metrics = cls_metrics)
```

## Tuning results

```{r, fig.width=10, fig.height=6}
autoplot(lr_res)
```

> The lower the penalty, the smaller the number of predictors used by the model. Such models should be preferred.

## Choose the best model

Here you see top 5 best models based on mean AUC and ranked by penalty score

```{r}
lr_res %>% 
  show_best("roc_auc", n = 5)
```

I will choose a model with the highest mean AUC

## ROC-AUC of the best model

```{r}
lr_res %>% 
  make_roc("LR") %>% 
  autoplot()

```

How many predictors left after removing covariates?

'raw' train data contains `r ncol(df_train)` columns.

```{r}
lr_recipe <- recipe(Diagnosis ~ ., data = df_train) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_predictors()) %>% 
  step_corr(threshold = 0.13) %>% # tuned threshold
  step_smote(Diagnosis, over_ratio = 1, seed = 100)

lr_recipe %>% prep() %>% juice() %>% ncol()
```

Nothing was removed in this case

# Tune and fit Random Forest models

I will use grid search to tune both RF hyper-parameters: `mtry` and `min_n`

> The `mtry` hyperparameter sets the number of predictor variables that each node in the decision tree "sees" and can learn about, so it can range from 1 to the total number of features present; when `mtry` = all possible features, the model is the same as bagging decision trees. The min_n hyperparameter sets the minimum n to split at any node

## Basic preprocessing recipe

```{r}
set.seed(5732)

# number of cores available on Kaggle
cores <- 8L 

# model specification
rf_mod <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode("classification")

# join model and processing recipe
rf_wf <- workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(basic_recipe)

# tune models, this takes time
rf_basic_res <- tune_grid(rf_wf,
            grid = 30,
            resamples = cv_folds,
            control = control_grid(save_pred = TRUE),
            metrics = cls_metrics)

```

### Tuning results

```{r}
autoplot(rf_basic_res)
```

It is better to choose `mtry` and `min_n` corresponding to higher ROC-AUC *and* J-index.

### Best models

5 best models ranked by mean AUC

```{r}
rf_basic_res %>% 
  show_best(metric = "roc_auc")
```

5 best models ranked by J-index

```{r}
rf_basic_res %>% 
  show_best(metric = "j_index")
```

### ROC-AUC of the best model

```{r}
rf_basic_res %>% make_roc("RF") %>% autoplot()
```

The ROC-curve looks very good: it reaches both Sensitivity/TPR = 1.0 *and* 1-Specificity/FPR = 1.0

Since there is no highly correlated predictors, there is no need in applying LR recipe.

## Comparison of j-index based and ROC-based best model configurations

On the tuning plot above you can see that the model with the highest j-index is not the same as the model with the highest ROC.

Do they really differ in performance?

### Posterior distributions of AUC values

I will compare the best models using Bayesian ANOVA

```{r}
# function to get a name of the best preprocessor from a resampling object
best_preprocessor <- function(res_obj, metric="roc_auc") {
  res_obj %>% select_best(metric) %>% pull(.config)
}

# function to make a df with AUCs from the best preprocessor
best_models <- function(res_obj, model_name, metric1="roc_auc", metric2="roc_auc"){
  res_obj %>%
    collect_metrics(summarize = FALSE) %>%
    filter(.metric == metric1, .config == best_preprocessor(res_obj, metric=metric2)) %>%
    select(id, id2, {{model_name}} := .estimate)
}

rf_best_auc <- best_models(rf_basic_res, model_name = "auc_auc", metric1 = "roc_auc", metric2 = "roc_auc")

rf_best_j <- best_models(rf_basic_res, model_name = "auc_j", metric1 = "roc_auc", metric2 = "j_index")

rf_comp <- list(rf_best_auc, rf_best_j) %>% 
  reduce(inner_join, by = c("id", "id2")) %>% 
  unite(id, id, id2)

rf_posterior <- perf_mod(rf_comp, iter = 2000, seed = 100, refresh = 0, chains = 5, cores = 2)

autoplot(rf_posterior)
```

### Estimate the Difference

```{r}
auc_diff <- contrast_models(rf_posterior, seed = 100) 

summary(auc_diff)
```

The difference in AUC between models chosen using J-index or ROC is insignificant.

# Tune and fit Boosted Trees model

Using the same recipe as for RF and Bayesian grid search

```{r}
set.seed(732)

# number of cores available on Kaggle
cores <- 8L 

# model specification
xgb_mod <- 
  boost_tree(
    trees = 50, 
    mtry = tune(), 
    min_n = tune(), 
    tree_depth = tune(), 
    learn_rate = tune(), 
    loss_reduction = tune(), 
    sample_size = tune(), 
    stop_iter = tune()) %>% 
  set_engine("xgboost", num.threads = cores) %>% 
  set_mode("classification")

# join model and processing recipe
xgb_cv_wf <- workflow() %>% 
  add_model(xgb_mod) %>% 
  add_recipe(basic_recipe)

# extract params for Bayesian grid search
param_set <- extract_parameter_set_dials(xgb_cv_wf) %>%
  finalize(x = df_train %>% select(-Diagnosis))

# tune models, this takes time
xgb_res <- tune_bayes(
  xgb_cv_wf,
  param_info = param_set,
  initial = 20,
  metrics = cls_metrics,
  resamples = cv_folds,
  iter = 50,
  control = control_bayes(
    no_improve = 30,
    verbose = FALSE,
    save_pred = TRUE,
    save_workflow = TRUE
  )
)
```

## Tuning results

```{r, fig.width=14, fig.height=4}
autoplot(xgb_res)
```

## Best models

5 best models ranked by mean AUC

```{r}
xgb_res %>% 
  show_best(metric = "roc_auc")
```

5 best models ranked by mean J-index

```{r}
xgb_res %>% 
  show_best(metric = "j_index")
```

The best hyper-parameters look like this:

```{r}
xgb_best <- xgb_res %>% 
  select_best(metric = "roc_auc")

xgb_best
```

## ROC-AUC of the best model

```{r}
xgb_res %>% make_roc("") %>% autoplot() + ggtitle("XGB")
```

# Compare Logistic Regression, Random Forest and Boosted Trees models

## ROCs

```{r}
roc_df <- map2_dfr(list(xgb_res, rf_basic_res, lr_res),
                            c("XGB", "RF", "LR"), 
                            function(x, y) make_roc(x, y))
roc_df %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1")

```

As expected, Boosted Trees (BT) model performs (slightly) better than RF, and both BT and RF perform significantly better than LR.

## Posterior distributions

```{r}
mod_comp <-
  map2(list(xgb_res, rf_basic_res, lr_res),
       c("xgb", "rf", "lr"),
       ~ best_aucs(res_obj = .x, model_name = .y)) %>% # model_names is not passed correctly
  reduce(inner_join, by = c("id", "id2")) %>% 
  rename("xgb" = .y.x, "rf" = .y.y, "lr" = .y) %>% 
  unite(id, id, id2)

mod_posterior <- perf_mod(mod_comp, iter = 2000, seed = 100, refresh = 0, chains = 5, cores = 4)

autoplot(mod_posterior)
```


```{r}
mod_diff <- contrast_models(mod_posterior, seed = 100) 

summary(mod_diff)
```

Difference between XGB and RF is statistically insignificant.

I choose XGB to apply it to the test data set.

# The final fit

Fit the BT model with chosen hyper-parameters to the entire training data set and test it on the test data set

```{r}
# the last model
last_xgb_mod <- 
  boost_tree(trees = 50,
             mtry = xgb_best$mtry, 
             min_n = xgb_best$min_n, 
             tree_depth = xgb_best$tree_depth,
             learn_rate = xgb_best$learn_rate,
             loss_reduction = xgb_best$loss_reduction,
             sample_size = xgb_best$sample_size,
             stop_iter = xgb_best$stop_iter) %>%
  set_engine("xgboost", num.threads = cores) %>% 
  set_mode("classification")

# the last workflow
last_xgb_wf <- 
  xgb_cv_wf %>% 
  update_model(last_xgb_mod)

# the last fit
set.seed(345)
last_xgb_fit <- 
  last_xgb_wf %>% 
  last_fit(data_split)
```

## Basic performance metrics on the test data set

```{r}
last_xgb_fit %>% 
  collect_metrics()
```

## Variable importance

```{r}
last_xgb_fit %>% 
  extract_fit_parsnip() %>% 
  vip(num_features = 12)
```

Interestingly, Sex does not play any role in prediction process.

## ROC curve on the test data set

```{r}
last_xgb_fit %>% 
  collect_predictions() %>% 
  roc_curve(Diagnosis, .pred_Hepatitis) %>% 
  autoplot()

```

Sensitivity reaches 1.0 while Specificity is still quite close to 1.0, which means that this model can predict all hepatitis cases and report low number of false positives at the same time.

## Balancing performance by choosing optimal probability cut-off.

I will use j-index (as explained [here](https://probably.tidymodels.org/articles/where-to-use.html)) to balance performance of the model.

> j-index has a maximum value of 1 when there are no false positives and no false negatives. It can be used as justification of whether or not an increase in the threshold value is worth it. If increasing the threshold results in more of an increase in the specificity than a decrease in the sensitivity, we can see that with j-index.

```{r}
# collect sens, spec, j-index at various cut-offs
threshold_data <- 
  last_xgb_fit %>%
  collect_predictions() %>%
  threshold_perf(Diagnosis, .pred_Hepatitis, thresholds = seq(0.0, 1, by = 0.05)) %>% 
  filter(.metric != "distance") %>%
  mutate(group = case_when(
    .metric == "sens" | .metric == "spec" ~ "1",
    TRUE ~ "2"
  ))

# find max j-index
max_j_index_threshold <- threshold_data %>%
  filter(.metric == "j_index") %>%
  filter(.estimate == max(.estimate)) %>%
  pull(.threshold)

# plot metrics v cut-offs
ggplot(threshold_data, aes(x = .threshold, y = .estimate, color = .metric, alpha = group)) +
  geom_line(size=1) +
  #theme_minimal() +
  #scale_color_viridis_d(end = 0.9) +
  scale_color_brewer(palette = "Set1") +
  scale_alpha_manual(values = c(.4, 1), guide = "none") +
  geom_vline(xintercept = max_j_index_threshold, alpha = .8, color = "grey30", linetype = "longdash") +
  labs(
    x = "Probability",
    y = "Metric Estimate",
    title = "Choosing the optimal probability cut-off"
  )
```

j-index is at its maximum at probability cut-off `r max_j_index_threshold`. This value will be chosen for final confusion matrix

## Confusion Matrix

```{r}
pred_optimized <- last_xgb_fit %>%
  collect_predictions() %>% 
  mutate(
    .pred = make_two_class_pred(
      estimate = .pred_Hepatitis, 
      levels = levels(Diagnosis), 
      threshold = max_j_index_threshold
    )
  ) %>%
  select(Diagnosis, contains(".pred"))

cm_optimized <- pred_optimized %>% 
  conf_mat(truth = Diagnosis, estimate = .pred)

autoplot(cm_optimized, type = "heatmap")
```

## All performance metrics

With probability cut-off `r max_j_index_threshold`

```{r}
summary(cm_optimized)
```

## Conclusion

From a clinical perspective, we should be interested both in high sensitivity and specificity, because we do not want to miss any patients with the sickness and at the same time we do not want to raise alarms falsely too many times. The extent to which we can accept lower levels of both metrics varies in each case and should be discussed. Current model's performance seems to be sufficiently high and balanced.

------------------------------------------------------------------------

## Save the workspace

```{r}
save.image("data/workspace.RData")
```
