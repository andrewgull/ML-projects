---
title: "Hepatits C prediction"
author: "A.G."
date-modified: "last update: `r format(Sys.Date(), format = '%d %B %Y')`"
format: 
  html:
    toc: true
    toc-depth: 2
    toc-title: Contents
    toc-location: left
    df-print: paged
    standalone: true
    code-fold: true
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F, cache = T)
library(skimr)
library(tidyverse)
library(caret)
# installed via archive: install.packages("/Path/to/DMwR_0.4.1.tar.gz", repos=NULL, type="source" )
# or 
# remotes::install_version("DMwR", version="0.4.1")
library(DMwR) 
library(ROCR)
library(tidymodels)
```

# Intro

The aim is to predict development of hepatitis' stages.

> The target attribute for classification is Category (2): blood donors vs. Hepatitis C patients (including its progress ('just' Hepatitis C, Fibrosis, Cirrhosis).

I'll join *Cirrhosis*, *Fibrosis* and *Hepatitis C* into one category **Hepatitis**, the other one will be called **Donor**

# Read data set

```{r}
df <- read_csv("data/HepatitisCdata.csv", col_types = "dfdfdddddddddd") %>% select(-...1)

# make re-coded Category variable
# there is 0=Blood Donor and 0s=suspect Blood Donor
df <- df %>% 
  mutate(Diagnosis = if_else(str_detect(Category, "Donor"), "Donor", "Hepatitis")) %>%
  mutate(Diagnosis = factor(Diagnosis, levels = c("Donor", "Hepatitis"))) %>%
  relocate(Diagnosis, .before = Category) %>%
  select(-Category)

head(df)
```

# Description

## Factor variables

```{r}
skim(df) %>%
  yank("factor")
```

No missing data here, but the target variable is imbalanced: 75 positive cases vs 540 negative cases

## Numeric variables

```{r}
skim(df) %>%
  yank("numeric")
```

Numeric variables are mostly complete. I will use imputation later to fill in the gaps

# EDA

## Pairs plot

```{r pairs, fig.width=14, fig.height=10, warning=FALSE, message=FALSE}
GGally::ggpairs(df, aes(color = Diagnosis), 
        upper = list(continuous = GGally::wrap("cor", size = 2.0)),
        diag = list(continuous = "barDiag"),
        lower = list(continuous = GGally::wrap("points", alpha = 0.3, size=0.8))) +
  scale_color_brewer(palette = "Set1", direction = -1) +
  scale_fill_brewer(palette = "Set1", direction = -1)
```

## Age v Diagnosis

```{r}
ggplot(df, aes(Diagnosis, Age)) +
  geom_violin(aes(fill = Diagnosis), alpha = 0.5) +
  geom_jitter(alpha = 0.5, size = 0.6) +
  geom_boxplot(aes(fill = Diagnosis), alpha = 0.2) +
  scale_fill_brewer(palette = "Set1", direction = -1) +
  xlab("")
```

## Age v ALB

```{r}
ggplot(df, aes(Diagnosis, ALB)) +
  geom_violin(aes(fill = Diagnosis), alpha = 0.5) +
  geom_jitter(alpha = 0.5, size = 0.6) +
  geom_boxplot(aes(fill = Diagnosis), alpha = 0.2) +
  scale_fill_brewer(palette = "Set1", direction = -1) +
  xlab("")
```

## ALP v Diagnosis

```{r}
ggplot(df, aes(Diagnosis, ALP)) +
  geom_violin(aes(fill = Diagnosis), alpha = 0.5) +
  geom_jitter(alpha = 0.5, size = 0.6) +
  geom_boxplot(aes(fill = Diagnosis), alpha = 0.2) +
  scale_fill_brewer(palette = "Set1", direction = -1) +
  xlab("")
```

## ALT v Diagnosis

```{r}
ggplot(df, aes(Diagnosis, ALT)) +
  geom_violin(aes(fill = Diagnosis), alpha = 0.5) +
  geom_jitter(alpha = 0.5, size = 0.6) +
  geom_boxplot(aes(fill = Diagnosis), alpha = 0.2) +
  scale_fill_brewer(palette = "Set1", direction = -1) +
  coord_trans(y = "sqrt") +
  xlab("")
```

## AST v Diagnosis

```{r}
ggplot(df, aes(Diagnosis, AST)) +
  geom_violin(aes(fill = Diagnosis), alpha = 0.5) +
  geom_jitter(alpha = 0.5, size = 0.6) +
  geom_boxplot(aes(fill = Diagnosis), alpha = 0.2) +
  scale_fill_brewer(palette = "Set1", direction = -1) +
  coord_trans(y = "sqrt") +
  xlab("")
```

## BIL v Diagnosis

```{r}
ggplot(df, aes(Diagnosis, BIL)) +
  geom_violin(aes(fill = Diagnosis), alpha = 0.5) +
  geom_jitter(alpha = 0.5, size = 0.6) +
  geom_boxplot(aes(fill = Diagnosis), alpha = 0.2) +
  scale_fill_brewer(palette = "Set1", direction = -1) +
  coord_trans(y = "sqrt") +
  xlab("")
```

## CHE v Diagnosis

```{r}
ggplot(df, aes(Diagnosis, CHE)) +
  geom_violin(aes(fill = Diagnosis), alpha = 0.5) +
  geom_jitter(alpha = 0.5, size = 0.6) +
  geom_boxplot(aes(fill = Diagnosis), alpha = 0.2) +
  scale_fill_brewer(palette = "Set1", direction = -1) +
  xlab("")
```

## CHOL v Diagnosis

```{r}
ggplot(df, aes(Diagnosis, CHOL)) +
  geom_violin(aes(fill = Diagnosis), alpha = 0.5) +
  geom_jitter(alpha = 0.5, size = 0.6) +
  geom_boxplot(aes(fill = Diagnosis), alpha = 0.2) +
  scale_fill_brewer(palette = "Set1", direction = -1) +
  xlab("")
```

## CREA v Diagnosis

```{r}
ggplot(df, aes(Diagnosis, CREA)) +
  geom_violin(aes(fill = Diagnosis), alpha = 0.5) +
  geom_jitter(alpha = 0.2, size = 0.3) +
  geom_boxplot(aes(fill = Diagnosis), alpha = 0.2) +
  scale_fill_brewer(palette = "Set1", direction = -1) +
  coord_trans(y = "sqrt") +
  xlab("")
```

## GGT v Diagnosis

```{r}
ggplot(df, aes(Diagnosis, GGT)) +
  geom_violin(aes(fill = Diagnosis), alpha = 0.5) +
  geom_jitter(alpha = 0.5, size = 0.6) +
  geom_boxplot(aes(fill = Diagnosis), alpha = 0.2) +
  scale_fill_brewer(palette = "Set1", direction = -1) +
  coord_trans(y = "sqrt") +
  xlab("")
```

Median of most numerical variables are different between `Donor` and `Hepatitis` groups.

## Sex v Diagnosis

```{r}
sex_n <- df %>% group_by(Diagnosis, Sex) %>% summarise(N = n())

ggplot(sex_n, aes(Diagnosis, N)) +
  geom_col(aes(fill = Sex), position = "fill") +
  scale_fill_brewer(palette = "Set2", direction = -1) +
  xlab("") +
  ylab("Proportion")
```

Difference in proportion of each sex is not that big between `Donor` and `Hepatitis` groups.

# Data processing

## Test use: Specification

Any imputation or oversamplig here?

```{r}
hep_rec <- recipe(Diagnosis ~ ., data = df_train_smote) %>%
  step_zv(all_predictors()) # to remove 0 disperse
# no update_role()
# no step_dummy()
```

Model engine

```{r}
lr_mod <- 
  logistic_reg() %>% 
  set_engine("glm")
```

Model workflow

```{r}
hep_wflow <- 
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(hep_rec)

hep_wflow
```

Train the actual model

```{r}
hep_fit <- 
  hep_wflow %>% 
  fit(data = df_train_smote)
```

Extract the fit object

```{r}
hep_fit %>% 
  extract_fit_parsnip() %>% 
  tidy()
```

Predict

```{r}
hep_aug <- 
  augment(hep_fit, df_test)

hep_aug
```

get ROC

```{r}
hep_aug %>% 
  roc_curve(truth = Diagnosis, .pred_Donor) %>% 
  autoplot()
```

```{r}
hep_aug %>% 
  roc_auc(truth = Diagnosis, .pred_Donor)
```


## Imputation

```{r, warning=FALSE, message=FALSE}
library(mice)

imp_mice <- mice(df)
df_imp <- complete(imp_mice)

```

## Dummy vars

`Sex` is the only categorical variable in this data set.

For tree based models it is [not necessary](https://bookdown.org/max/FES/categorical-trees.html) to turn categorical variables (factors) to dummy variables.

```{r, include=FALSE, eval=FALSE}
to_dum <- df_imp %>% select(Sex)
# make an obj
dummies <- dummyVars(~ ., data = to_dum)
# apply it
df_dummy <- data.frame(predict(dummies, newdata = to_dum))

head(df_dummy)
```

## Scaling and centering

```{r}
df_proc <- df_imp %>% 
  select(-c(Sex, Diagnosis)) %>% 
  scale() %>% 
  data.frame() %>% 
  mutate(Sex = df$Sex, Diagnosis = df$Diagnosis)

head(df_proc)
```

# Split data into training and testing

```{r data.split}
set.seed(124)

data_split <- initial_split(df_proc, prop = 3/4)

df_train <- training(data_split)
df_test <- testing(data_split)
```

## Target class equalization in training data set

Initial target class counts:

```{r}
df_train %>%
  count(Diagnosis)
```

To make the target class balanced, I use Synthetic Minority Oversampling Technique (SMOTE) to create training data set, but not testing one. 

Target class now looks like this:

```{r smote}
df_train_smote <- SMOTE(Diagnosis ~ ., data.frame(df_train), perc.over = 400, perc.under = 160)

df_train_smote %>%
  count(Diagnosis)
```

Now the target variable is more balanced.

# Random Forest

## Using parsnip

```{r}
set.seed(56982)
rf_with_seed %>% 
  set_args(mtry = 4) %>% 
  set_engine("randomForest") %>%
  fit(Diagnosis ~ ., data = df_train_smote)
```


## Training

5-fold CV repeated 10 times

```{r rf.train}
set.seed(120)

fit_ctrl <- trainControl(## 5-fold CV
                           method = "repeatedcv",
                           number = 5,
                           repeats = 10, 
                           allowParallel = T,
                           classProbs = T,
                           summaryFunction = twoClassSummary)

fit_rf <- train(Diagnosis ~ ., 
                 data = df_train_smote, 
                 metric = "ROC", 
                 method = "rf", 
                 trControl = fit_ctrl,
                 tuneGrid = expand.grid(.mtry = seq(2, 12, 0.5)),
                 ntree = 10,
                 nodesize = 1,
                 verbosity = 0,
                 verbose = FALSE)

fit_rf

```

## Features Importance

```{r featimp.rf}
imp_vars_rf <- varImp(fit_rf)

plot(imp_vars_rf, main = "Variable Importance with RF")

```

## Testing

Here's a function to summarize a model's performance, it is used below.

```{r roc.fun}
get_roc <- function(fit.obj, testing.df){
  pred_prob <- predict.train(fit.obj, newdata = testing.df, type = "prob")
  pred_roc <- prediction(predictions = pred_prob$Hepatitis, labels = testing.df$Diagnosis)
  perf_roc <- performance(pred_roc, measure = "tpr", x.measure = "fpr")
  return(list(perf_roc, pred_roc))
}
```

### ROC curve

RF's performance on the testing data set

```{r roc.rf}
# calculate ROC
perf_pred <- get_roc(fit_rf, df_test)
perf_rf <- perf_pred[[1]]
pred_rf <- perf_pred[[2]]

# take AUC 
auc_rf <- round(unlist(slot(performance(pred_rf, measure = "auc"), "y.values")), 3)

# plot
plot(perf_rf, main = "RF ROC curve", col = "steelblue", lwd = 3)
abline(a = 0, b = 1, lwd = 3, lty = 2, col = 1)
legend(x = 0.7, y = 0.3, legend = paste0("AUC = ", auc_rf))

```

AUC is pretty close to 1.0, True Positive Rate close to 1.0 while False Positive Rate is low

### TPR v FPR

```{r tpr.rf}
# choose your cut-off
cutoff <- 0.08

# use pred_rf (pred_roc) object
plot(performance(pred_rf, measure = "tpr", x.measure = "cutoff"),
     col="steelblue", 
     ylab = "Rate", 
     xlab="Probability cutoff")

plot(performance(pred_rf, measure = "fpr", x.measure = "cutoff"), 
     add = T, col = "red")

legend(x = 0.6,y = 0.7, c("TPR (Recall)", "FPR (1-Spec)"), 
       lty = 1, col =c('steelblue', 'red'), bty = 'n', cex = 1, lwd = 2)

#abline(v = cutoff, lwd = 2, lty=6)

title("RF")
```

Unfortunately TPR falls quickly even at low probability cutoff, which means it's hard to 'catch' all cases with hepatitis without 'catching' any false positive predictions.

Let's have a look at the confusion matrix.

### Confusion matrix

With selected cutoff

```{r cm.rf}
pred_prob_rf <- predict(fit_rf, newdata = df_test, type = "prob")

# turn probabilities into classes
pred_class_rf <- ifelse(pred_prob_rf$Hepatitis > cutoff, "Hepatitis", "Donor")

pred_class_rf <- as.factor(pred_class_rf)

cm_rf <- confusionMatrix(data = pred_class_rf, 
                reference = df_test$Diagnosis,
                mode = "everything",
                positive = "Hepatitis")

cm_rf
```

We see. that almost all hepatitis cases have been predicted but we still have a lot of false positives which makes PPV quite low \~0.5

In other words, if our model predicts a patient to have hepatitis, there is \~50% chance that the patient doesn't have it.

Also, the model should be able to capture **all** cases with the disease - that's what is required in clinical practice, i.e Sensitivity/Recall should be 1.0

To overcome these two problems, a better algorithm should be used or the current one should be tuned better.

# Boosting

## Train

```{r xgb.train, eval=T}
set.seed(121)
# basic grid search
fit_xgb <- train(Diagnosis ~ ., 
                 data = df_train_smote, 
                 method = "xgbTree",
                 metric = "ROC", 
                 trControl = fit_ctrl,
                 tuneLength = 5,
                 nthreads = 12,
                 verbose = FALSE,
                 verbosity = 0)

fit_xgb$bestTune
```

## Features importance

```{r featimp.xgb, eval=T}
imp_vars_xgb <- varImp(fit_xgb)

plot(imp_vars_xgb, main = "Variable Importance with XGB")
```

## Test

### ROC

```{r roc.xgb, eval=T}
# calculate ROC
perf_pred <- get_roc(fit_xgb, df_test)
perf_xgb <- perf_pred[[1]]
pred_xgb <- perf_pred[[2]]

# take AUC 
auc_xgb <- round(unlist(slot(performance(pred_xgb, measure = "auc"), "y.values")), 3)

# plot
plot(perf_xgb, main = "XGB ROC curve", col = "steelblue", lwd = 3)
abline(a = 0, b = 1, lwd = 3, lty = 2, col = 1)
legend(x = 0.7, y = 0.3, legend = paste0("AUC = ", auc_xgb))

```

### TPR v FPR

```{r tpr.xgb, eval=T}
cutoff <- 0.12

# use pred_rf (pred_roc) object
plot(performance(pred_xgb, measure = "tpr", x.measure = "cutoff"),
     col="steelblue", 
     ylab = "Rate", 
     xlab="Probability cutoff")

plot(performance(pred_xgb, measure = "fpr", x.measure = "cutoff"), 
     add = T, col = "red")

legend(x = 0.6,y = 0.7, c("TPR (Recall)", "FPR (1-Spec)"), 
       lty = 1, col =c('steelblue', 'red'), bty = 'n', cex = 1, lwd = 2)

abline(v = cutoff, lwd = 2, lty=6)

title("XGB")
```

### Confusion matrix with selected cutoff

```{r cm.xgb, eval=T}
pred_prob_xgb <- predict(fit_xgb, newdata = df_test, type = "prob")

# turn probabilities into classes
pred_class_xgb <- ifelse(pred_prob_xgb$Hepatitis > cutoff, "Hepatitis", "Donor")

pred_class_xgb <- as.factor(pred_class_xgb)

cm_xgb <- confusionMatrix(data = pred_class_xgb, 
                reference = df_test$Diagnosis,
                mode = "everything",
                positive = "Hepatitis")

cm_xgb
```

------------------------------------------------------------------------

## Save the workspace

```{r}
save.image("data/workspace.RData")
```
