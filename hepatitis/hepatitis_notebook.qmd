---
title: "Hepatits C prediction using R and tidymodels"
author: "A.G."
date-modified: "last update: `r format(Sys.Date(), format = '%d %B %Y')`"
format: 
  html:
    toc: true
    toc-depth: 2
    toc-title: Contents
    toc-location: left
    df-print: paged
    standalone: true
    code-fold: true
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F, cache = T)
library(skimr)
library(readr)
library(stringr)
library(tidymodels)
library(themis) # for SMOTE and other recipes for target balancing
library(vip) # for variable importance
```

# Intro

The aim is to predict development of hepatitis' stages.

> The target attribute for classification is Category (2): blood donors vs. Hepatitis C patients (including its progress ('just' Hepatitis C, Fibrosis, Cirrhosis).

I'll join *Cirrhosis*, *Fibrosis* and *Hepatitis C* into one category **Hepatitis**, the other one will be called **Donor**

# Read data set

The data set looks like this:

```{r}
df <- read_csv("data/HepatitisCdata.csv", col_types = "dfdfdddddddddd") %>% select(-...1)

# make re-coded Category variable
# there is 0=Blood Donor and 0s=suspect Blood Donor
df <- df %>% 
  mutate(Diagnosis = if_else(str_detect(Category, "Donor"), "Donor", "Hepatitis")) %>%
  mutate(Diagnosis = factor(Diagnosis, levels = c("Hepatitis", "Donor"))) %>%
  relocate(Diagnosis, .before = Category) %>%
  select(-Category)

head(df)
```

# Descriptive statistics

## Factor variables

```{r}
skim(df) %>%
  yank("factor")
```

No missing data here, but the target variable is imbalanced: 75 positive cases vs 540 negative cases

## Numeric variables

```{r}
skim(df) %>%
  yank("numeric")
```

Numeric variables are mostly complete. I will use imputation later to fill in the gaps

# EDA

## Pairs plot

```{r pairs, fig.width=14, fig.height=10, warning=FALSE, message=FALSE}
GGally::ggpairs(df, aes(color = Diagnosis), 
        upper = list(continuous = GGally::wrap("cor", size = 2.0)),
        diag = list(continuous = "barDiag"),
        lower = list(continuous = GGally::wrap("points", alpha = 0.3, size=0.8))) +
  scale_color_brewer(palette = "Set1", direction = -1) +
  scale_fill_brewer(palette = "Set1", direction = -1)
```

## Age v Diagnosis

```{r}
ggplot(df, aes(Diagnosis, Age)) +
  geom_violin(aes(fill = Diagnosis), alpha = 0.5) +
  geom_jitter(alpha = 0.5, size = 0.6) +
  geom_boxplot(aes(fill = Diagnosis), alpha = 0.2) +
  scale_fill_brewer(palette = "Set1", direction = -1) +
  xlab("")
```

## Age v ALB

```{r}
ggplot(df, aes(Diagnosis, ALB)) +
  geom_violin(aes(fill = Diagnosis), alpha = 0.5) +
  geom_jitter(alpha = 0.5, size = 0.6) +
  geom_boxplot(aes(fill = Diagnosis), alpha = 0.2) +
  scale_fill_brewer(palette = "Set1", direction = -1) +
  xlab("")
```

## ALP v Diagnosis

```{r}
ggplot(df, aes(Diagnosis, ALP)) +
  geom_violin(aes(fill = Diagnosis), alpha = 0.5) +
  geom_jitter(alpha = 0.5, size = 0.6) +
  geom_boxplot(aes(fill = Diagnosis), alpha = 0.2) +
  scale_fill_brewer(palette = "Set1", direction = -1) +
  xlab("")
```

## ALT v Diagnosis

```{r}
ggplot(df, aes(Diagnosis, ALT)) +
  geom_violin(aes(fill = Diagnosis), alpha = 0.5) +
  geom_jitter(alpha = 0.5, size = 0.6) +
  geom_boxplot(aes(fill = Diagnosis), alpha = 0.2) +
  scale_fill_brewer(palette = "Set1", direction = -1) +
  coord_trans(y = "sqrt") +
  xlab("")
```

## AST v Diagnosis

```{r}
ggplot(df, aes(Diagnosis, AST)) +
  geom_violin(aes(fill = Diagnosis), alpha = 0.5) +
  geom_jitter(alpha = 0.5, size = 0.6) +
  geom_boxplot(aes(fill = Diagnosis), alpha = 0.2) +
  scale_fill_brewer(palette = "Set1", direction = -1) +
  coord_trans(y = "sqrt") +
  xlab("")
```

## BIL v Diagnosis

```{r}
ggplot(df, aes(Diagnosis, BIL)) +
  geom_violin(aes(fill = Diagnosis), alpha = 0.5) +
  geom_jitter(alpha = 0.5, size = 0.6) +
  geom_boxplot(aes(fill = Diagnosis), alpha = 0.2) +
  scale_fill_brewer(palette = "Set1", direction = -1) +
  coord_trans(y = "sqrt") +
  xlab("")
```

## CHE v Diagnosis

```{r}
ggplot(df, aes(Diagnosis, CHE)) +
  geom_violin(aes(fill = Diagnosis), alpha = 0.5) +
  geom_jitter(alpha = 0.5, size = 0.6) +
  geom_boxplot(aes(fill = Diagnosis), alpha = 0.2) +
  scale_fill_brewer(palette = "Set1", direction = -1) +
  xlab("")
```

## CHOL v Diagnosis

```{r}
ggplot(df, aes(Diagnosis, CHOL)) +
  geom_violin(aes(fill = Diagnosis), alpha = 0.5) +
  geom_jitter(alpha = 0.5, size = 0.6) +
  geom_boxplot(aes(fill = Diagnosis), alpha = 0.2) +
  scale_fill_brewer(palette = "Set1", direction = -1) +
  xlab("")
```

## CREA v Diagnosis

```{r}
ggplot(df, aes(Diagnosis, CREA)) +
  geom_violin(aes(fill = Diagnosis), alpha = 0.5) +
  geom_jitter(alpha = 0.2, size = 0.3) +
  geom_boxplot(aes(fill = Diagnosis), alpha = 0.2) +
  scale_fill_brewer(palette = "Set1", direction = -1) +
  coord_trans(y = "sqrt") +
  xlab("")
```

## GGT v Diagnosis

```{r}
ggplot(df, aes(Diagnosis, GGT)) +
  geom_violin(aes(fill = Diagnosis), alpha = 0.5) +
  geom_jitter(alpha = 0.5, size = 0.6) +
  geom_boxplot(aes(fill = Diagnosis), alpha = 0.2) +
  scale_fill_brewer(palette = "Set1", direction = -1) +
  coord_trans(y = "sqrt") +
  xlab("")
```

Median of most numerical variables are different between `Donor` and `Hepatitis` groups.

## Sex v Diagnosis

```{r}
sex_n <- df %>% group_by(Diagnosis, Sex) %>% summarise(N = n())

ggplot(sex_n, aes(Diagnosis, N)) +
  geom_col(aes(fill = Sex), position = "fill") +
  scale_fill_brewer(palette = "Set2", direction = -1) +
  xlab("") +
  ylab("Proportion")
```

Difference in proportion of each sex is not that big between `Donor` and `Hepatitis` groups.

# Data processing

The following analysis is based on tidyverse tutorials and examples [here](https://www.tidymodels.org/start/recipes/), [here](https://www.tidymodels.org/start/resampling/), [here](https://www.tidymodels.org/learn/work/tune-svm/).

## Split

```{r data.split}
set.seed(124)

data_split <- initial_split(df, prop = 3/4, strata = Diagnosis)

df_train <- training(data_split)
df_test <- testing(data_split)

# make a validation set
# to test a model on 0.8 of the test data set, validate on 0.2
val_set <- validation_split(df_train, 
                            strata = Diagnosis,
                            prop = 0.80)
```

## Preprocessing

- Impute using median

- Turn nominal variables into dummies (it is not required by tree-based models or logistic regression, but without this step oversampling step won't work)

- Normalize all predictors

- Balance target classes using SMOTE algorithm

```{r}
my_recipe <- recipe(Diagnosis ~ ., data = df_train) %>%
  step_impute_median(all_numeric_predictors()) %>% # I use median bcz not that many observations are missing
  step_dummy(all_nominal_predictors()) %>% # dummy goes before normalisation
  step_normalize(all_predictors()) %>% 
  step_smote(Diagnosis, over_ratio = 1, seed = 100) %>% # original target distribution 399 v 62
  check_missing(all_predictors())

my_recipe
```

```{r, include=FALSE, echo=FALSE}
# the other way is to apply the recipe to your data immediately
# prep & bake
train_data <- my_recipe %>% 
  prep(training = df_train) %>% 
  bake(new_data = NULL) # df_train will be processed

# bake test. what about SMOTE?
test_data <- my_recipe %>% 
  prep( training = df_test) %>% 
  bake(new_data = df_test)

# check oversampling results
train_data %>% count(Diagnosis) # SMOTE was applied
test_data %>% count(Diagnosis) # not applied
```


# Tune and fit Random Forest models

Stratified, repeated 10-fold cross-validation is used

ROC and J-index will be used as maximization metrics.

>If a model is poorly calibrated, the ROC curve value might not show diminished performance. However, the J index would be lower for models with pathological distributions for the class probabilities. 

```{r}
set.seed(5732)

# number of cores available on Kaggle
cores <- 4L 

# model specification
rf_mod <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode("classification")

# join model and processing recipe
rm_cv_wf <- workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(my_recipe)

# Stratified, repeated 10-fold cross-validation
cv_folds <- vfold_cv(df_train, strata = "Diagnosis", repeats = 5)

# metrics
cls_metrics <- metric_set(roc_auc, j_index)

# tune models, this takes time
rf_res <- tune_grid(rm_cv_wf,
            grid = 25,
            resamples = cv_folds,
            control = control_grid(save_pred = TRUE),
            metrics = cls_metrics)

# look at the models' ROC
autoplot(rf_res)
```

# Tune and fit penalized logistic regression

Package [glmnet](https://glmnet.stanford.edu/articles/glmnet.html) will be used.

Recipe for this model includes removal of correlated variables (correlation threshold = 0.75)

```{r}
# recipe for LR
lr_recipe <- recipe(Diagnosis ~ ., data = df_train) %>%
  step_impute_median(all_numeric_predictors()) %>% # I use median bcz not that many observations are missing
  step_dummy(all_nominal_predictors()) %>% # dummy goes before normalisation
  step_normalize(all_predictors()) %>% 
  step_corr(threshold = 0.75) %>%
  step_smote(Diagnosis, over_ratio = 1, seed = 100) # original target distribution 399 v 62

# set model type/engine
lr_mod <- logistic_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet")

# define the workflow
lr_workflow <- 
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(lr_recipe)

# create a tune grid
lr_reg_grid <- tibble(penalty = 10**seq(-4, 0, length.out = 30))

# train and tune the model
lr_res <- tune_grid(lr_workflow,
              grid = lr_reg_grid,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE),
              metrics = cls_metrics)

lr_plot <- lr_res %>% 
  collect_metrics() %>% 
  ggplot(aes(x = penalty, y = mean)) + 
  geom_point() + 
  geom_line() + 
  ylab("Area under the ROC Curve") +
  scale_x_log10(labels = scales::label_number())

lr_plot
```

Best models

```{r}
top_models <-
  lr_res %>% 
  show_best("roc_auc", n = 15) %>% 
  arrange(penalty) 

top_models %>% arrange(penalty)
```

the lower the penalty, the smaller the number of predictors used by the model. Such models should be preferred.

Let's take the model 19 from the table above

```{r}
lr_best <- 
  lr_res %>% 
  collect_metrics() %>% 
  arrange(penalty) %>% 
  slice(12)
lr_best
```



```{r}
lr_auc <- 
  lr_res %>% 
  collect_predictions(parameters = lr_best) %>% 
  roc_curve(Diagnosis, .pred_Hepatitis) %>% 
  mutate(model = "Logistic Regression")

autoplot(lr_auc)

```


Random forest

```{r}
# cores
cores <- parallel::detectCores() # 8
cores <- 6L

# model
rf_mod <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode("classification")

# preprocessing recipe
# there is no need in normalising and making dummies, but I'll make dummy for Sex variable to use SMOTE
# only imputation and oversampling
rf_recipe <- 
  recipe(Diagnosis ~ ., data = df_train) %>% 
  step_impute_median(all_numeric_predictors()) %>% # I use median bcz not that many observations are missing
  step_dummy(all_nominal_predictors()) %>% 
  step_smote(Diagnosis, over_ratio = 1, seed = 100) # original target distribution 399 v 62 

# workflow
rf_workflow <- 
  workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(rf_recipe)


```

The `mtry` hyperparameter sets the number of predictor variables that each node in the decision tree “sees” and can learn about, so it can range from 1 to the total number of features present; when `mtry` = all possible features, the model is the same as bagging decision trees. The min_n hyperparameter sets the minimum n to split at any node

```{r}
# tune
set.seed(345)

rf_res <- 
  rf_workflow %>% 
  tune_grid(val_set,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))
#> i Creating pre-processing data to finalize unknown parameter: mtry

rf_res %>% 
  show_best(metric = "roc_auc")
```

```{r}
autoplot(rf_res)
```

```{r}
rf_best <- 
  rf_res %>% 
  select_best(metric = "roc_auc")
rf_best
```

```{r}
rf_auc <- 
  rf_res %>% 
  collect_predictions(parameters = rf_best) %>% 
  roc_curve(Diagnosis, .pred_Hepatitis) %>% 
  mutate(model = "Random Forest")
rf_auc
```

Compare LR and RF

```{r}
bind_rows(rf_auc, lr_auc) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 1, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_viridis_d(option = "plasma", end = .6)

```


The last fit

```{r}
# the last model
last_rf_mod <- 
  rand_forest(mtry = 1, min_n = 38, trees = 1000) %>% # this comes from the best chosen model
  set_engine("ranger", num.threads = cores, importance = "impurity") %>% 
  set_mode("classification")

# the last workflow
last_rf_workflow <- 
  rf_workflow %>% 
  update_model(last_rf_mod)

# the last fit
set.seed(345)
last_rf_fit <- 
  last_rf_workflow %>% 
  last_fit(data_split)

last_rf_fit

```


On test set

```{r}
last_rf_fit %>% 
  collect_metrics()
```

Variable importance

```{r}
last_rf_fit %>% 
  extract_fit_parsnip() %>% 
  vip(num_features = 20)

```

```{r}
last_rf_fit %>% 
  collect_predictions() %>% 
  roc_curve(Diagnosis, .pred_Hepatitis) %>% 
  autoplot()

```


## Imputation

```{r, warning=FALSE, message=FALSE}
library(mice)

imp_mice <- mice(df)
df_imp <- complete(imp_mice)

```

## Dummy vars

`Sex` is the only categorical variable in this data set.

For tree based models it is [not necessary](https://bookdown.org/max/FES/categorical-trees.html) to turn categorical variables (factors) to dummy variables.

```{r, include=FALSE, eval=FALSE}
to_dum <- df_imp %>% select(Sex)
# make an obj
dummies <- dummyVars(~ ., data = to_dum)
# apply it
df_dummy <- data.frame(predict(dummies, newdata = to_dum))

head(df_dummy)
```

## Scaling and centering

```{r}
df_proc <- df_imp %>% 
  select(-c(Sex, Diagnosis)) %>% 
  scale() %>% 
  data.frame() %>% 
  mutate(Sex = df$Sex, Diagnosis = df$Diagnosis)

head(df_proc)
```

# Split data into training and testing

```{r data.split}
set.seed(124)

data_split <- initial_split(df_proc, prop = 3/4)

df_train <- training(data_split)
df_test <- testing(data_split)
```

## Target class equalization in training data set

Initial target class counts:

```{r}
df_train %>%
  count(Diagnosis)
```

To make the target class balanced, I use Synthetic Minority Oversampling Technique (SMOTE) to create training data set, but not testing one. 

Target class now looks like this:

```{r smote}
df_train_smote <- SMOTE(Diagnosis ~ ., data.frame(df_train), perc.over = 400, perc.under = 160)

df_train_smote %>%
  count(Diagnosis)
```

Now the target variable is more balanced.

# Random Forest

## Using parsnip

```{r}
set.seed(56982)
rf_with_seed %>% 
  set_args(mtry = 4) %>% 
  set_engine("randomForest") %>%
  fit(Diagnosis ~ ., data = df_train_smote)
```


## Training

5-fold CV repeated 10 times

```{r rf.train}
set.seed(120)

fit_ctrl <- trainControl(## 5-fold CV
                           method = "repeatedcv",
                           number = 5,
                           repeats = 10, 
                           allowParallel = T,
                           classProbs = T,
                           summaryFunction = twoClassSummary)

fit_rf <- train(Diagnosis ~ ., 
                 data = df_train_smote, 
                 metric = "ROC", 
                 method = "rf", 
                 trControl = fit_ctrl,
                 tuneGrid = expand.grid(.mtry = seq(2, 12, 0.5)),
                 ntree = 10,
                 nodesize = 1,
                 verbosity = 0,
                 verbose = FALSE)

fit_rf

```

## Features Importance

```{r featimp.rf}
imp_vars_rf <- varImp(fit_rf)

plot(imp_vars_rf, main = "Variable Importance with RF")

```

## Testing

Here's a function to summarize a model's performance, it is used below.

```{r roc.fun}
get_roc <- function(fit.obj, testing.df){
  pred_prob <- predict.train(fit.obj, newdata = testing.df, type = "prob")
  pred_roc <- prediction(predictions = pred_prob$Hepatitis, labels = testing.df$Diagnosis)
  perf_roc <- performance(pred_roc, measure = "tpr", x.measure = "fpr")
  return(list(perf_roc, pred_roc))
}
```

### ROC curve

RF's performance on the testing data set

```{r roc.rf}
# calculate ROC
perf_pred <- get_roc(fit_rf, df_test)
perf_rf <- perf_pred[[1]]
pred_rf <- perf_pred[[2]]

# take AUC 
auc_rf <- round(unlist(slot(performance(pred_rf, measure = "auc"), "y.values")), 3)

# plot
plot(perf_rf, main = "RF ROC curve", col = "steelblue", lwd = 3)
abline(a = 0, b = 1, lwd = 3, lty = 2, col = 1)
legend(x = 0.7, y = 0.3, legend = paste0("AUC = ", auc_rf))

```

AUC is pretty close to 1.0, True Positive Rate close to 1.0 while False Positive Rate is low

### TPR v FPR

```{r tpr.rf}
# choose your cut-off
cutoff <- 0.08

# use pred_rf (pred_roc) object
plot(performance(pred_rf, measure = "tpr", x.measure = "cutoff"),
     col="steelblue", 
     ylab = "Rate", 
     xlab="Probability cutoff")

plot(performance(pred_rf, measure = "fpr", x.measure = "cutoff"), 
     add = T, col = "red")

legend(x = 0.6,y = 0.7, c("TPR (Recall)", "FPR (1-Spec)"), 
       lty = 1, col =c('steelblue', 'red'), bty = 'n', cex = 1, lwd = 2)

#abline(v = cutoff, lwd = 2, lty=6)

title("RF")
```

Unfortunately TPR falls quickly even at low probability cutoff, which means it's hard to 'catch' all cases with hepatitis without 'catching' any false positive predictions.

Let's have a look at the confusion matrix.

### Confusion matrix

With selected cutoff

```{r cm.rf}
pred_prob_rf <- predict(fit_rf, newdata = df_test, type = "prob")

# turn probabilities into classes
pred_class_rf <- ifelse(pred_prob_rf$Hepatitis > cutoff, "Hepatitis", "Donor")

pred_class_rf <- as.factor(pred_class_rf)

cm_rf <- confusionMatrix(data = pred_class_rf, 
                reference = df_test$Diagnosis,
                mode = "everything",
                positive = "Hepatitis")

cm_rf
```

We see. that almost all hepatitis cases have been predicted but we still have a lot of false positives which makes PPV quite low \~0.5

In other words, if our model predicts a patient to have hepatitis, there is \~50% chance that the patient doesn't have it.

Also, the model should be able to capture **all** cases with the disease - that's what is required in clinical practice, i.e Sensitivity/Recall should be 1.0

To overcome these two problems, a better algorithm should be used or the current one should be tuned better.

# Boosting

## Train

```{r xgb.train, eval=T}
set.seed(121)
# basic grid search
fit_xgb <- train(Diagnosis ~ ., 
                 data = df_train_smote, 
                 method = "xgbTree",
                 metric = "ROC", 
                 trControl = fit_ctrl,
                 tuneLength = 5,
                 nthreads = 12,
                 verbose = FALSE,
                 verbosity = 0)

fit_xgb$bestTune
```

## Features importance

```{r featimp.xgb, eval=T}
imp_vars_xgb <- varImp(fit_xgb)

plot(imp_vars_xgb, main = "Variable Importance with XGB")
```

## Test

### ROC

```{r roc.xgb, eval=T}
# calculate ROC
perf_pred <- get_roc(fit_xgb, df_test)
perf_xgb <- perf_pred[[1]]
pred_xgb <- perf_pred[[2]]

# take AUC 
auc_xgb <- round(unlist(slot(performance(pred_xgb, measure = "auc"), "y.values")), 3)

# plot
plot(perf_xgb, main = "XGB ROC curve", col = "steelblue", lwd = 3)
abline(a = 0, b = 1, lwd = 3, lty = 2, col = 1)
legend(x = 0.7, y = 0.3, legend = paste0("AUC = ", auc_xgb))

```

### TPR v FPR

```{r tpr.xgb, eval=T}
cutoff <- 0.12

# use pred_rf (pred_roc) object
plot(performance(pred_xgb, measure = "tpr", x.measure = "cutoff"),
     col="steelblue", 
     ylab = "Rate", 
     xlab="Probability cutoff")

plot(performance(pred_xgb, measure = "fpr", x.measure = "cutoff"), 
     add = T, col = "red")

legend(x = 0.6,y = 0.7, c("TPR (Recall)", "FPR (1-Spec)"), 
       lty = 1, col = c('steelblue', 'red'), bty = 'n', cex = 1, lwd = 2)

abline(v = cutoff, lwd = 2, lty= 6)

title("XGB")
```

### Confusion matrix with selected cutoff

```{r cm.xgb, eval=T}
pred_prob_xgb <- predict(fit_xgb, newdata = df_test, type = "prob")

# turn probabilities into classes
pred_class_xgb <- ifelse(pred_prob_xgb$Hepatitis > cutoff, "Hepatitis", "Donor")

pred_class_xgb <- as.factor(pred_class_xgb)

cm_xgb <- confusionMatrix(data = pred_class_xgb, 
                reference = df_test$Diagnosis,
                mode = "everything",
                positive = "Hepatitis")

cm_xgb
```

------------------------------------------------------------------------

## Save the workspace

```{r}
save.image("data/workspace.RData")
```
