---
title: "House Prices"
author: "AG"
format: html
---

# Libraries

```{r, message=FALSE}
library(themis)
library(probably)
library(vip)
library(bestNormalize) # for ord QQ norm
library(embed) # for UMAP
library(tidyverse)
library(tidymodels)
library(finetune) # fro win-loss tuning
library(tidyposterior)
```

# EDA

## First look at the data 

```{r}
data <- read_csv("data/train.csv", show_col_types = FALSE) 

data_submission <- read_csv("data/test.csv", show_col_types = FALSE) %>% 
  mutate(across(is_character, ~ as_factor(.x)))

dim(data)
```

I should submit price predictions for the test data set

## Factors

Here I will look at character columns and convert them into factors after NA replacement

```{r}
data %>% 
  skim() %>% 
  yank("character")
```

Notes on NAs:

- According to `data_description.txt`, NA in `Alley` means "no alley access" - should be replaced by "no access"

- NA in `MasVnrType` means "no data"

- NA in `BsmtQual`means "no basement" - should be replaced

- NA in `BsmtCond` means "no basement" - should be replaced

- NA in `BsmtExposure` means "no basement" - should be replaced

- NA in `BsmtFinType1` means "no basement" - should be replaced

- NA in `BsmtFinType2`means "no basement" - should be replaced

- NA in `Electrical` means "no data"

- NA in `FireplaceQu` means "no fireplace", should be replaced

- NA in `Garage*` means "no garage" -  should be replaced

- NA in `PoolQC` means "no pool" - should be replaced

- NA in `Fence` means "no fence" - should be replaced

- NA in `MiscFeature` means "no data"


### Replacing some NAs and converting to factors

```{r}
data <- data %>%
  mutate_at(
    vars(
      Alley,
      BsmtQual,
      BsmtCond,
      BsmtExposure,
      BsmtFinType1,
      BsmtFinType2,
      FireplaceQu,
      GarageType,
      GarageCond,
      GarageFinish,
      GarageCars,
      GarageQual,
      PoolQC,
      Fence
    ),
    ~ replace_na(., "No")
  ) %>%
  mutate(across(is_character, ~ as_factor(.x)))

data %>% 
  skimr::skim() %>% 
  skimr::yank("factor")
```


## Numerics

```{r}
data %>% 
  skimr::skim() %>% 
  skimr::yank("numeric")
```

For NA imputation I will use some kind of algorithm

### Raw distributions

```{r, fig.width=18, fig.height=18}
make_dens_plot <- function(sub_df) {
  ggplot(sub_df, aes(value)) +
    geom_density() +
    xlab("") +
    ggtitle((sub_df %>% pull(predictor))[1]) +
    theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = 0.5, size = 8))
}

dens_plot_list <- data %>%
  select(where(is.numeric), -Id) %>%
  pivot_longer(cols = where(is.numeric),
               names_to = "predictor",
               values_to = "value") %>%
  group_by(predictor) %>%
  group_split() %>%
  map(~ make_dens_plot(.x))

ggpubr::ggarrange(plotlist = dens_plot_list, ncol = 7, nrow = 6)
```

### After Yeo-Johnson transformation

```{r, fig.width=18, fig.height=18}
yj_recipe <- recipe(SalePrice ~ ., data = select(data, where(is.numeric), -Id)) %>%
  update_role(SalePrice, new_role = "outcome") %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_YeoJohnson(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors())

dens_yj_plot_list <- 
  yj_recipe %>% 
  prep() %>% 
  juice() %>%
  pivot_longer(cols = where(is.numeric),
               names_to = "predictor",
               values_to = "value") %>%
  group_by(predictor) %>%
  group_split() %>%
  map( ~ make_dens_plot(.x))

ggpubr::ggarrange(plotlist = dens_yj_plot_list, ncol = 7, nrow = 6)
```


# Corr. matrix for numerical predictors

```{r, fig.width=10, fig.height=10}
cor_matrix <- data %>% 
  select(where(is.numeric)) %>%
  cor(use = "pairwise.complete.obs", method = "spearman")

corrplot::corrplot(cor_matrix, type = "upper", tl.col = "black", tl.cex = 0.6, method = "ellipse")
```

Some predictors are correlated with each other.

`SalePrice` (outcome) is correlated with quite a lot of the predictors.

# Boxplots for factors

```{r, fig.width=18, fig.height=18, warning=FALSE}
make_plot <- function(sub_df) {
  ggplot(sub_df, aes(value, SalePrice)) +
    geom_boxplot(varwidth = T) +
    geom_violin(alpha = 0.1) +
    xlab("") +
    ggtitle((sub_df %>% pull(predictor))[1]) +
    theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = 0.5, size = 8))
}

plot_list <- data %>%
  select(where(is.factor), SalePrice) %>%
  pivot_longer(cols = where(is.factor),
               names_to = "predictor",
               values_to = "value") %>%
  group_by(predictor) %>%
  group_split() %>%
  map(~ make_plot(.x))

ggpubr::ggarrange(plotlist =  plot_list, ncol = 6, nrow = 8)
```


# PCA

## Recipe

```{r}
pca_recipe <- recipe(SalePrice ~ ., data = data) %>%
  update_role(Id, new_role = "ID") %>%
  update_role(SalePrice, new_role = "outcome") %>% 
  step_nzv(all_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>%
  step_impute_mean(all_numeric_predictors()) %>% 
  step_impute_knn(all_nominal_predictors()) %>%
  step_YeoJohnson(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_pca(all_predictors(), threshold = .9) %>% 
  step_normalize(all_numeric_predictors())

data_pca <- pca_recipe %>% prep() %>% juice()

data_pca <- prep(pca_recipe, retain = TRUE)

data_pca$template %>% dim()
```

## Explained variation

### Table

```{r}
sdev <- data_pca$steps[[8]]$res$sdev
percent_variation <- sdev^2 / sum(sdev^2)

var_df <- data.frame(PC = paste0("PC", 1:length(sdev)),
                     var_explained = percent_variation,
                     stringsAsFactors = FALSE)

var_df <- var_df %>% 
  mutate(var_cum_sum = cumsum(var_explained))

var_df
```


### Plot

40 PCs explaining 0.9 of the variance

```{r, fig.width=6, fig.height=3}
var_df %>%
  filter(var_cum_sum < .903) %>% 
  mutate(PC = forcats::fct_inorder(PC)) %>%
  ggplot(aes(x = PC, y = var_explained)) + 
  geom_col(aes(fill = var_cum_sum)) +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.8, hjust = 0.5, size = 8)) +
  scale_fill_continuous(type = "viridis")
```


## Scatter plot

```{r, warning=FALSE, fig.width=7}
library(ggforce)

plot_dimred_results <- function(dat, color_by = SalePrice) {
  dat %>%
    select(-Id) %>% 
    select(contains(c("01", "02", "03")), {{color_by}}) %>% 
    # Create the scatterplot matrix
    ggplot(aes(x = .panel_x, y = .panel_y, color = {{color_by}})) +
    geom_point(alpha = 0.4, size = 1) +
    geom_autodensity(alpha = .3) +
    facet_matrix(vars(-{{color_by}}), layer.diag = 2) 
}

data_pca$template %>%
  plot_dimred_results() +
  ggtitle("Principal Component Analysis")
```

# UMAP

```{r, fig.width=6, warning=FALSE}
decorq_recipe <- recipe(SalePrice ~ ., data = data) %>%
  update_role(Id, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_impute_mean(all_numeric_predictors()) %>% 
  step_impute_knn(all_nominal_predictors()) %>% 
  step_YeoJohnson(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_best_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_corr(threshold = 0.75)

umap_recipe <- decorq_recipe %>%
  step_umap(
    all_numeric_predictors(),
    num_comp = 10,
    min_dist = 0.01,
    neighbors = 25
  )

data_umap <- prep(umap_recipe, retain = TRUE)

data_umap$template %>% 
  plot_dimred_results() +
  ggtitle("UMAP")
```


# Preprocessing

Some predictors are in fact discrete (like `OverallQual`) - will this affect models?

## Data split

```{r}
set.seed(124)

data_split <- initial_split(data, prop = 0.7)

df_train <- training(data_split)
df_test <- testing(data_split)

dim(df_train)
```


```{r}
dim(df_test)
```


## Folds and metrics

```{r}
set.seed(124)
cv_folds <- vfold_cv(df_train, v = 5, repeats = 10) 

cls_metrics <- metric_set(rmse)
```


## Recipes

### Base, PCA, decorr & decorq

```{r}
base_recipe <- recipe(SalePrice ~ ., data = df_train) %>%
  update_role(Id, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_impute_mean(all_numeric_predictors()) %>% 
  step_impute_knn(all_nominal_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors())

pca_yj_recipe <- recipe(SalePrice ~ ., data = data) %>%
  update_role(Id, new_role = "ID") %>%
  update_role(SalePrice, new_role = "outcome") %>% 
  step_nzv(all_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>%
  step_impute_mean(all_numeric_predictors()) %>% 
  step_impute_knn(all_nominal_predictors()) %>%
  step_YeoJohnson(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_pca(all_predictors(), num_comp = tune()) %>% 
  step_normalize(all_numeric_predictors())

decorr_yj_recipe <- recipe(SalePrice ~ ., data = df_train) %>%
  update_role(Id, new_role = "ID") %>%
  update_role(SalePrice, new_role = "outcome") %>%
  step_nzv(all_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_YeoJohnson(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>%
  step_corr(threshold = tune("corr_tune")) # The step will try to remove the minimum number of columns so that all the resulting absolute correlations are less than this value

decorr_recipe <- recipe(SalePrice ~ ., data = df_train) %>%
  update_role(Id, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_impute_mean(all_numeric_predictors()) %>% 
  step_impute_knn(all_nominal_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_corr(threshold = tune("corr_tune"))

decorq_recipe <- recipe(SalePrice ~ ., data = df_train) %>%
  update_role(Id, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_impute_mean(all_numeric_predictors()) %>% 
  step_impute_knn(all_nominal_predictors()) %>% 
  step_orderNorm(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_corr(threshold = tune("corr_tune"))
```


### With feature selection

```{r}
library(recipeselectors)

feat_sel_recipe <- recipe(SalePrice ~ ., data = df_train) %>%
  update_role(Id, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_impute_mean(all_numeric_predictors()) %>% 
  step_impute_knn(all_nominal_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_select_vip(all_predictors(), model = rf_mod, outcome = "SalePrice", threshold = 0.9)

```


# Modelling

## Basic LR

### Decorrelated predictors

```{r}
set.seed(133)
# model specs
# mixture = 1 specifies a pure lasso model,
# mixture = 0 specifies a ridge regression model, and
# ⁠0 < mixture < 1⁠ specifies an elastic net model, interpolating lasso and ridge
lr_mod <- linear_reg(penalty = tune(),
                     mixture = tune()) %>%
  set_engine("glmnet", num.threads = 6) %>%
  set_mode("regression")

# model's workflow
lr_wf <- workflow() %>%
  add_model(lr_mod) %>%
  add_recipe(decorr_recipe)

# tuning and resampling
lr_decorr_res <- lr_wf %>%
  tune_grid(
    grid = 20,
    resamples = cv_folds,
    control = control_grid(save_pred = TRUE,
                           save_workflow = TRUE),
    metrics = cls_metrics
  )

#lr_decorr_res <- readRDS("data/lr_decorr_res.rds")
#saveRDS(lr_decorr_res, "data/lr_decorr_res.rds")
autoplot(lr_decorr_res)
```

#### Best models

```{r}
lr_decorr_res %>% 
  show_best()
```

### Decorrelated predictors + YJ

```{r}
set.seed(133)

# model's workflow
lr_wf <- workflow() %>%
  add_model(lr_mod) %>%
  add_recipe(decorr_yj_recipe)

# tuning and resampling
lr_decorr_yj_res <- lr_wf %>%
  tune_grid(
    grid = 20,
    resamples = cv_folds,
    control = control_grid(save_pred = TRUE,
                           save_workflow = TRUE),
    metrics = cls_metrics
  )


#saveRDS(lr_decorr_yj_res, "data/lr_decorr_yj_res.rds")
autoplot(lr_decorr_yj_res)
```

## LR PCA recipe

tuned number of components

```{r}
lr_pca_wf <- workflow() %>%
  add_model(lr_mod) %>%
  add_recipe(pca_yj_recipe)

lr_pca_res <- lr_pca_wf %>%
  tune_grid(
    grid = 20,
    resamples = cv_folds,
    control = control_grid(save_pred = TRUE,
                           save_workflow = TRUE),
    metrics = cls_metrics
  )

#saveRDS(lr_pca_res, "data/lr_pca_res")
autoplot(lr_pca_res)
```

#### Best models

```{r}
show_best(lr_pca_res)
```

## Basic RF

basic pre-processing recipe & space-filled grid search

```{r}
# model specification
rf_mod <- rand_forest(mtry = tune(),
                      min_n = tune(),
                      trees = 1000) %>%
  set_engine("ranger", num.threads = 6) %>%
  set_mode("regression")

# model workflow
rf_wf <- workflow() %>%
  add_model(rf_mod) %>%
  add_recipe(base_recipe)

# tuning and resampling
rf_base_res <- rf_wf %>%
  tune_grid(
    grid = 20,
    resamples = cv_folds,
    control = control_grid(save_pred = TRUE,
                           save_workflow = TRUE),
    metrics = cls_metrics
  )

#rf_base_res <- readRDS("data/rf_res.rds")
#saveRDS(rf_base_res, "data/rf_base_res")
autoplot(rf_base_res)
```

### Best models

```{r}
rf_base_res %>% 
  show_best("rmse")
```

## MLP

### PCA

```{r}
mlp_mod <-
  mlp(hidden_units = tune(),
      penalty = tune(),
      epochs = tune()) %>%
  set_mode("regression") %>%
  set_engine("nnet", num.threads = 8)

mlp_wf <- workflow() %>% 
  add_model(mlp_mod) %>% 
  add_recipe(pca_recipe)

mlp_res <- mlp_wf %>%
  tune_grid(
    grid = 20,
    resamples = cv_folds,
    control = control_grid(save_pred = TRUE,
                           save_workflow = TRUE),
    metrics = cls_metrics
  )

autoplot(mlp_res)
```

```{r}
show_best(mlp_res)
```


### Decorrelated predictors & ORQ transformation

#### Recipe

```{r}
decorq_na_recipe <- recipe(SalePrice ~ ., data = df_train) %>%
  update_role(Id, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_filter_missing(all_predictors()) %>% 
  step_impute_mean(all_numeric_predictors()) %>% 
  step_impute_knn(all_nominal_predictors()) %>% 
  step_orderNorm(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_lincomb(all_predictors()) %>% 
  step_corr(threshold = tune("corr_tune"))
```

#### Tuning

```{r}
mlp_decorq_wf <- workflow() %>% 
  add_model(mlp_mod) %>% 
  add_recipe(decorq_na_recipe)

mlp_decorq_res <- mlp_decorq_wf %>%
  tune_grid(
    grid = 20,
    resamples = cv_folds,
    control = control_grid(save_pred = TRUE,
                           save_workflow = TRUE),
    metrics = cls_metrics
  )

```

#### Autoplot

```{r}
autoplot(mlp_decorq_res)
```

#### Best models

```{r}
mlp_decorq_res %>% show_best()
```

## XGB

### Boosted trees specification

```{r}
xgb_spec <- boost_tree(
  trees = 50,
  mtry = tune(),
  min_n = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune(),
  sample_size = tune(),
  stop_iter = tune()
) %>%
  set_engine("xgboost", num.threads = 8) %>%
  set_mode("regression")
```


### Basic preprocessing and win-loss grid search

```{r}
xgb_wf <- workflow() %>% 
  add_model(xgb_spec) %>% 
  add_recipe(base_recipe)

param_set <- extract_parameter_set_dials(xgb_wf) %>%
  finalize(x = df_train %>% select(-SalePrice))

xgb_wl_res <- xgb_wf %>%
  tune_race_win_loss(
    param_info = param_set,
    resamples = cv_folds,
    grid = 40,
    control = control_race(
      verbose_elim = TRUE,
      save_pred = TRUE,
      save_workflow = TRUE,
      burn_in = 10
    )
  )

# xgb_wl_res <- readRDS("data/xgb_wl_res.rds")
# saveRDS(xgb_wl_res, "data/xgb_wl_res.rds")
plot_race(xgb_wl_res)
```

#### Best models

```{r}
xgb_wl_res %>% 
  show_best("rmse")
```


### Decorrelated recipe and win-loss search

```{r}
# xgb specification is the same as above
xgb_wf <- workflow() %>% 
  add_model(xgb_spec) %>% 
  add_recipe(decorr_recipe)

param_set <- extract_parameter_set_dials(xgb_wf) %>%
    finalize(x = df_train %>% select(-SalePrice))

xgb_decorr_wl_res <- xgb_wf %>%
  tune_race_win_loss(
    param_info = param_set,
    resamples = cv_folds,
    grid = 20,
    control = control_race(
      verbose_elim = TRUE,
      save_pred = TRUE,
      save_workflow = TRUE,
      burn_in = 10
    )
  )

saveRDS(xgb_decorr_wl_res, "data/xgb_decorr_wl_res.rds")
# xgb_decorr_wl_res <- readRDS("data/xgb_decorr_wl_res")
plot_race(xgb_decorr_wl_res)
```

#### Best models

```{r}
xgb_decorr_wl_res %>% 
  show_best("rmse")
```


### Basic recipe and full Bayesian search

30 iterations

```{r}
xgb_wf <- workflow() %>% 
  add_model(xgb_spec) %>% 
  add_recipe(base_recipe)

param_set <- extract_parameter_set_dials(xgb_wf) %>%
    finalize(x = df_train %>% select(-SalePrice))

xgb_bres <- xgb_wf %>%
    tune_bayes(
      resamples = cv_folds,
      param_info = param_set,
      initial = 20,
      iter = 30,
      metrics = cls_metrics,
      control = control_bayes(
        no_improve = 20,
        verbose = FALSE,
        save_pred = TRUE,
        save_workflow = TRUE
      )
    )

#saveRDS(xgb_bres, "data/xgb_base_bres.rds")

xgb_bres <- readRDS("data/xgb_base_bres.rds")

xgb_bres %>% 
  show_best("rmse")
```

#### Autoplot

```{r, fig.width=5}
autoplot(xgb_bres)
```


### Decorrelated recipe and Bayesian search

30 iterations

```{r}
xgb_wf <- workflow() %>% 
  add_model(xgb_spec) %>% 
  add_recipe(decorr_recipe)

param_set <- extract_parameter_set_dials(xgb_wf) %>%
    finalize(x = df_train %>% select(-SalePrice))

xgb_dec_bres <- xgb_wf %>%
    tune_bayes(
      resamples = cv_folds,
      param_info = param_set,
      initial = 20,
      iter = 30,
      metrics = cls_metrics,
      control = control_bayes(
        no_improve = 20,
        verbose = FALSE,
        save_pred = TRUE,
        save_workflow = TRUE
      )
    )

#saveRDS(xgb_dec_bres, "data/xgb_dec_bres.rds")
#xgb_dec_bres <- readRDS("data/xgb_dec_bres.rds")

xgb_dec_bres %>% 
  show_best("rmse")
```

#### Autoplot

```{r, fig.width=7}
autoplot(xgb_dec_bres)
```


# Compare mean RMSE between different model specifications

```{r}
# function to retrieve the best HP combination based on mean RMSE
best_rmse <- function(res_obj, name){
  res_obj %>% 
    show_best("rmse") %>% 
    select(mean, std_err) %>%
    mutate(model = name) %>% 
    slice_head(n = 1)
}

# make a data frame
best_models_rmse <-
  map2_dfr(
    list(
      rf_base_res,
      lr_decorr_res,
      lr_pca_res,
      #mlp_res,
      #mlp_decorq_res,
      xgb_wl_res,
      xgb_bres,
      xgb_dec_bres
    ),
    c(
      "RF basic",
      "LR dec",
      "LR pca",
      #"MLP pca",
      #"MLP no corr ORQ",
      "XGB wl",
      "XGB Bayes",
      "XGB dec Bayes"
    ),
    ~ best_rmse(.x, .y)
  )

ggplot(best_models_rmse,
       aes(
         y = mean,
         x = model,
         ymin = mean - std_err,
         ymax = mean + std_err
       )) +
  geom_errorbar(aes(color = model)) +
  geom_point(aes(y = mean, color = model)) +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.95, hjust=1)) +
  xlab("") +
  ylab("RMSE")
```

## Bayesian ANOVA

```{r}
library(tidyposterior)

# function to get a name of the best preprocessor from your resample
best_preprocessor <- function(res_obj) {
  res_obj %>% 
    select_best("rmse") %>% 
    pull(.config)
}

# function to make a df with AUCs from the best preproc
best_rmse <- function(res_obj, model_name){
  res_obj %>%
    collect_metrics(summarize = FALSE) %>%
    filter(.metric == "rmse", .config == best_preprocessor(res_obj)) %>%
    select(id, id2, {{model_name}} := .estimate)
}

# generate distributions
mod_names <- c("RF", "LR", "XGB wl", "XGB full")
resampling_comparison <-
  map2(list(rf_base_res, lr_decorr_res, xgb_wl_res, xgb_bres),
       mod_names,
       ~ best_rmse(res_obj = .x, model_name = .y)) %>% 
  reduce(inner_join, by = c("id", "id2")) %>% 
  set_names(c("id", "id2", mod_names)) %>% 
  unite(id, id, id2)

# make ANOVA
resampling_posterior <-
  perf_mod(
    resampling_comparison,
    iter = 10000,
    seed = 100,
    refresh = 0,
    chains = 4,
    cores = 4
  )

# plot
autoplot(resampling_posterior)
```

### Estimate the difference

```{r}
res_diff <- contrast_models(resampling_posterior, seed = 100) 

summary(res_diff)
```

XGB has significantly better RMSE

win loss gives the same results as the full XGB

# Final fit

 emulates the process where, after determining the best model, 
 the final fit on the entire training set is needed 
 and is then evaluated on the test set.

```{r}
xgb_spec <- boost_tree(
  trees = 50,
  mtry = tune(),
  min_n = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune(),
  sample_size = tune(),
  stop_iter = tune()
) %>%
  set_engine("xgboost", num.threads = 8) %>%
  set_mode("regression")

# best model
best_mod <- xgb_dec_bres %>% 
  select_best("rmse")

# use hyper parameters of the best model
last_mod <-
  boost_tree(
    mtry = best_mod$mtry,
    min_n = best_mod$min_n,
    tree_depth = best_mod$tree_depth,
    learn_rate = best_mod$learn_rate,
    loss_reduction = best_mod$loss_reduction,
    sample_size = best_mod$sample_size,
    stop_iter = best_mod$stop_iter
  ) %>% 
  set_engine("xgboost", num.threads=8) %>% 
  set_mode("regression") 

# update corr threshold in the chosen recipe
decorr_recipe$steps[[6]] <-
  recipes::update(decorr_recipe$steps[[6]], threshold = best_mod$corr_tune)

xgb_wf <- workflow() %>% 
  add_model(xgb_spec) %>% 
  add_recipe(decorr_recipe) # the recipe here should be the one that was used for this model

# the last workflow
last_wf <- 
  xgb_wf %>%  
  update_model(last_mod)

# the last fit
set.seed(345)

final_fit <- 
  last_wf %>% 
  last_fit(data_split)


final_fit %>% 
  collect_metrics()
```

