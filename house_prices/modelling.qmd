---
title: "House Prices"
author: "AG"
format: html
---

# Libraries

```{r, message=FALSE}
library(themis)
library(probably)
library(vip)
library(bestNormalize) # for ord QQ norm
library(embed) # for UMAP
library(tidyverse)
library(tidymodels)
library(finetune) # fro win-loss tuning
library(tidyposterior)
```

# EDA

## First look at the data 

```{r}
data <- read_csv("data/train.csv", show_col_types = FALSE) 

dim(data)
```

I should submit price predictions for the test data set

## Factors

Here I will look at character columns and convert them into factors after NA replacement

```{r}
data %>% 
  skimr::skim() %>% 
  skimr::yank("character")
```

Notes on NAs:

- According to `data_description.txt`, NA in `Alley` means "no alley access" - should be replaced by "no access"

- NA in `MasVnrType` means "no data"

- NA in `BsmtQual`means "no basement" - should be replaced

- NA in `BsmtCond` means "no basement" - should be replaced

- NA in `BsmtExposure` means "no basement" - should be replaced

- NA in `BsmtFinType1` means "no basement" - should be replaced

- NA in `BsmtFinType2`means "no basement" - should be replaced

- NA in `Electrical` means "no data"

- NA in `FireplaceQu` means "no fireplace", should be replaced

- NA in `Garage*` means "no garage" -  should be replaced

- NA in `PoolQC` means "no pool" - should be replaced

- NA in `Fence` means "no fence" - should be replaced

- NA in `MiscFeature` means "no data"


### Replacing some NAs and converting to factors

```{r}
data <- data %>%
  mutate_at(
    vars(
      Alley,
      BsmtQual,
      BsmtCond,
      BsmtExposure,
      BsmtFinType1,
      BsmtFinType2,
      FireplaceQu,
      GarageType,
      GarageCond,
      GarageFinish,
      GarageQual,
      PoolQC,
      Fence
    ),
    ~ replace_na(., "No")
  ) %>%
  mutate(across(is_character, ~ as_factor(.x)))

data %>%
  skimr::skim() %>% 
  skimr::yank("factor")
```


## Numerics

```{r}
data %>% 
  skimr::skim() %>% 
  skimr::yank("numeric")
```

`MSSubClass` is not numeric. Transfrom it to factor

```{r}
data <- data %>% 
  mutate(MSSubClass = as.factor(MSSubClass))
```


### Raw distributions

```{r, fig.width=18, fig.height=18}
make_dens_plot <- function(sub_df) {
  ggplot(sub_df, aes(value)) +
    geom_density() +
    xlab("") +
    ggtitle((sub_df %>% pull(predictor))[1]) +
    theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = 0.5, size = 8))
}

dens_plot_list <- data %>%
  select(where(is.numeric), -Id) %>%
  pivot_longer(cols = where(is.numeric),
               names_to = "predictor",
               values_to = "value") %>%
  group_by(predictor) %>%
  group_split() %>%
  map(~ make_dens_plot(.x))

ggpubr::ggarrange(plotlist = dens_plot_list, ncol = 7, nrow = 6)
```

### After Yeo-Johnson transformation

```{r, fig.width=18, fig.height=18}
yj_recipe <- recipe(SalePrice ~ ., data = select(data, where(is.numeric), -Id)) %>%
  update_role(SalePrice, new_role = "outcome") %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_YeoJohnson(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors())

dens_yj_plot_list <- 
  yj_recipe %>% 
  prep() %>% 
  juice() %>%
  pivot_longer(cols = where(is.numeric),
               names_to = "predictor",
               values_to = "value") %>%
  group_by(predictor) %>%
  group_split() %>%
  map( ~ make_dens_plot(.x))

ggpubr::ggarrange(plotlist = dens_yj_plot_list, ncol = 7, nrow = 6)
```

### log1p transform

```{r, fig.width=18, fig.height=18}
make_dens_plot_log <- function(sub_df) {
  ggplot(sub_df, aes(log1p(value))) +
    geom_density() +
    xlab("") +
    ggtitle((sub_df %>% pull(predictor))[1]) +
    theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = 0.5, size = 8))
}

dens_log1p_plot_list <- data %>% 
  select(where(is.numeric),-Id) %>%
  pivot_longer(cols = where(is.numeric),
               names_to = "predictor",
               values_to = "value") %>%
  group_by(predictor) %>%
  group_split() %>%
  map( ~ make_dens_plot_log(.x))

ggpubr::ggarrange(plotlist = dens_log1p_plot_list, ncol = 7, nrow = 6)
```


### Distribution of SalePrice

```{r}
data %>% 
  ggplot(aes(SalePrice)) + 
  geom_histogram(bins = 100)
```

Logarithm of `SalePrice`

```{r}
data %>% 
  ggplot(aes(log(SalePrice))) + 
  geom_histogram(bins = 100)
```


## Corr. plot

```{r, fig.width=10, fig.height=10}
cor_matrix <- data %>% 
  select(where(is.numeric)) %>%
  cor(use = "pairwise.complete.obs", method = "spearman")

corrplot::corrplot(cor_matrix, type = "upper", tl.col = "black", tl.cex = 0.6, method = "ellipse")
```

### Predictors correlated with `SalePrice`:

```{r}
cor_df <- cor_matrix %>% 
  as_tibble() %>% 
  select(SalePrice) %>%
  rename(corr.coeff = SalePrice) %>% 
  mutate(predictor = row.names(cor_matrix)) %>% 
  arrange(-abs(corr.coeff)) %>% 
  filter(predictor != "SalePrice", predictor != "Id")

cor_df
```

If we take predictors with correlations 0.5 and higher?

### The best correlated predictors

```{r}
very_basic_predictors <- cor_df %>% 
  filter(corr.coeff > 0.5) %>% 
  pull(predictor)

very_basic_predictors
```


## Boxplots for factors

```{r, fig.width=18, fig.height=18, warning=FALSE}
make_plot <- function(sub_df) {
  ggplot(sub_df, aes(value, SalePrice)) +
    geom_boxplot(varwidth = T) +
    geom_violin(alpha = 0.1) +
    xlab("") +
    ggtitle((sub_df %>% pull(predictor))[1]) +
    theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = 0.5, size = 8))
}

plot_list <- data %>%
  select(where(is.factor), SalePrice) %>%
  pivot_longer(cols = where(is.factor),
               names_to = "predictor",
               values_to = "value") %>%
  group_by(predictor) %>%
  group_split() %>%
  map(~ make_plot(.x))

ggpubr::ggarrange(plotlist =  plot_list, ncol = 6, nrow = 8)
```


## PCA

### Recipe

```{r}
pca_recipe <- recipe(SalePrice ~ ., data = data) %>%
  update_role(Id, new_role = "ID") %>%
  update_role(SalePrice, new_role = "outcome") %>% 
  step_nzv(all_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>%
  step_impute_mean(all_numeric_predictors()) %>% 
  step_impute_knn(all_nominal_predictors()) %>%
  step_YeoJohnson(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_pca(all_predictors(), threshold = .8) %>% 
  step_normalize(all_numeric_predictors())

data_pca <- pca_recipe %>% prep() %>% juice()

data_pca <- prep(pca_recipe, retain = TRUE)

data_pca$template %>% dim()
```

### Explained variation

#### Table

```{r}
sdev <- data_pca$steps[[8]]$res$sdev
percent_variation <- sdev^2 / sum(sdev^2)

var_df <- data.frame(PC = paste0("PC", 1:length(sdev)),
                     var_explained = percent_variation,
                     stringsAsFactors = FALSE)

var_df <- var_df %>% 
  mutate(var_cum_sum = cumsum(var_explained))

var_df
```


#### Plot

20 PCs explaining 0.8 of the variance

```{r, fig.width=6, fig.height=3}
var_df %>%
  filter(var_cum_sum < .81) %>% 
  mutate(PC = forcats::fct_inorder(PC)) %>%
  ggplot(aes(x = PC, y = var_explained)) + 
  geom_col(aes(fill = var_cum_sum)) +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.8, hjust = 0.5, size = 8)) +
  scale_fill_continuous(type = "viridis")
```


### Scatter plot

```{r, warning=FALSE, fig.width=7}
library(ggforce)

plot_dimred_results <- function(dat, color_by = SalePrice) {
  dat %>%
    select(-Id) %>% 
    select(contains(c("01", "02", "03")), {{color_by}}) %>% 
    # Create the scatterplot matrix
    ggplot(aes(x = .panel_x, y = .panel_y, color = {{color_by}})) +
    geom_point(alpha = 0.4, size = 1) +
    geom_autodensity(alpha = .3) +
    facet_matrix(vars(-{{color_by}}), layer.diag = 2) 
}

data_pca$template %>%
  plot_dimred_results() +
  ggtitle("Principal Component Analysis")
```

## UMAP

```{r, fig.width=6, warning=FALSE}
decorq_recipe <- recipe(SalePrice ~ ., data = data) %>%
  update_role(Id, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_impute_mean(all_numeric_predictors()) %>% 
  step_impute_knn(all_nominal_predictors()) %>% 
  step_YeoJohnson(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_best_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_corr(threshold = 0.75)

umap_recipe <- decorq_recipe %>%
  step_umap(
    all_numeric_predictors(),
    num_comp = 10,
    min_dist = 0.01,
    neighbors = 25
  )

data_umap <- prep(umap_recipe, retain = TRUE)

data_umap$template %>% 
  plot_dimred_results() +
  ggtitle("UMAP")
```

# Feature engineering

Let's make new features!

Idea in the followinf chunk is borrowed from [here](https://www.kaggle.com/code/alexdrozd/500-100-score-0-11565)

```{r}
data <- data %>% 
  mutate(Sold1 = YrSold - YearRemodAdd,
         Sold2 = YearRemodAdd - YearBuilt,
         Square = `1stFlrSF` + `2ndFlrSF` + TotalBsmtSF,
         Square1 = WoodDeckSF + OpenPorchSF + EnclosedPorch + `3SsnPorch` + ScreenPorch,
         Bath = BsmtFullBath + FullBath
         )
```

# Data split

```{r}
set.seed(124)

data_split <- initial_split(data, prop = 0.95)

df_train <- training(data_split)
df_test <- testing(data_split)

nrow(df_train)
```


```{r}
nrow(df_test)
```


# Folds and metrics

```{r}
set.seed(124)
cv_folds <- vfold_cv(df_train, v = 10, repeats = 10) 

# OR use the entire data set (doesn't help)
#cv_folds <- vfold_cv(data, v = 10, repeats = 10)

cls_metrics <- metric_set(rmse)
```

# Preprocessing


Some predictors are in fact discrete (like `OverallQual`) - will this affect models?

## Recipes

```{r}
base_recipe <- recipe(SalePrice ~ ., data = df_train) %>%
  update_role(Id, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_impute_mean(all_numeric_predictors()) %>% 
  step_impute_knn(all_nominal_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors())

base_log_recipe <- base_recipe %>% 
  step_log("SalePrice")

decorr_yj_recipe <- recipe(SalePrice ~ ., data = df_train) %>%
  update_role(Id, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_impute_mean(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_YeoJohnson(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors(), threshold = tune("corr_tune")) %>%
  step_log("SalePrice", offset = 1)

decorr_recipe <- recipe(SalePrice ~ ., data = df_train) %>%
  update_role(Id, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_impute_mean(all_numeric_predictors()) %>% 
  step_impute_mode(all_nominal_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>% 
  step_corr(all_numeric_predictors(), threshold = tune("corr_tune")) %>%
  step_log("SalePrice")

decorr_log_recipe <- recipe(SalePrice ~ ., data = df_train) %>%
  update_role(Id, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_impute_mean(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_log(all_numeric_predictors(), offset = 1) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors(), threshold = tune("corr_tune")) %>%
  step_log("SalePrice")

# FEATURE ELIMINATION
# my fork of colino package with fixed issue #1
if (!require(colino, quietly=TRUE))
  devtools::install_github("https://github.com/andrewgull/colino")

# create a RFE model
rfe_model <- rand_forest(mode = "regression") %>%
  set_engine("ranger", num.threads = 8, importance = "impurity")

# make a recipe with RFE based on random forest
base_rfe_recipe <- base_recipe %>%
  step_select_vip(
    all_predictors(),
    model = rfe_model,
    outcome = "SalePrice",
    threshold = 0.9
  )

# check how it works
base_rfe_recipe %>% 
  prep() %>% 
  juice() %>% 
  dim()
```


# Modelling

## Basic LASSO LR

mixture = 1 specifies a pure lasso model

mixture = 0 specifies a ridge regression model

⁠0 < mixture < 1⁠ specifies an elastic net model, interpolating lasso and ridge


### Decorrelated predictors

```{r}
resamples_file <- "data/lrl_decorr_res_cv10_new_split.rds"

lrl_spec <- linear_reg(penalty = tune(),
                      mixture = tune()) %>%
  set_engine("glmnet", num.threads = 4) %>%
  set_mode("regression")

if (file.exists(resamples_file)) {
  print("Found resamples file!")
  lrl_decorr_res <- readRDS(resamples_file)
} else {
  # model's workflow
  lrl_wf <- workflow() %>%
    add_model(lrl_spec) %>%
    add_recipe(decorr_recipe)
  
  # tuning and resampling
  lrl_decorr_res <- lrl_wf %>%
    tune_grid(
      grid = 20,
      resamples = cv_folds,
      control = control_grid(save_pred = TRUE,
                             save_workflow = TRUE),
      metrics = cls_metrics
    )
  saveRDS(lrl_decorr_res, resamples_file)
}

autoplot(lrl_decorr_res)
```

```{r}
lrl_decorr_res %>% 
  show_best()
```

### Decorrelated predictors + YJ transformation 


```{r}
resamples_file <- "data/lrl_decorr_yj_res_cv10_new_split.rds"

lrl_spec <- linear_reg(penalty = tune(),
                      mixture = tune()) %>%
  set_engine("glmnet", num.threads = 4) %>%
  set_mode("regression")

if (file.exists(resamples_file)) {
  print("Found resamples file!")
  lrl_decorr_yj_res <- readRDS(resamples_file)
} else {
  # model's workflow
  lrl_wf <- workflow() %>%
    add_model(lrl_spec) %>%
    add_recipe(decorr_yj_recipe)
  
  # tuning and resampling
  lrl_decorr_yj_res <- lrl_wf %>%
    tune_grid(
      grid = 20,
      resamples = cv_folds,
      control = control_grid(save_pred = TRUE,
                             save_workflow = TRUE),
      metrics = cls_metrics
    )
  saveRDS(lrl_decorr_yj_res, resamples_file)
}

autoplot(lrl_decorr_yj_res)
```

```{r}
show_best(lrl_decorr_yj_res)
```

### Decorrelated predictors + log1p

```{r}
resamples_file <- "data/lrl_decorr_log1p_res_cv10_new_split_mssub.rds"
#resamples_file <- "data/lrl_decorr_log1p_res_cv10_whole_data.rds"

lrl_spec <- linear_reg(penalty = tune(),
                      mixture = tune()) %>%
  set_engine("glmnet", num.threads = 4) %>%
  set_mode("regression")

if (file.exists(resamples_file)) {
  print("Found resamples file!")
  lrl_decorr_log1p_res <- readRDS(resamples_file)
} else {
  # model's workflow
  lrl_wf <- workflow() %>%
    add_model(lrl_spec) %>%
    add_recipe(decorr_log_recipe)
  
  # tuning and resampling
  lrl_decorr_log1p_res <- lrl_wf %>%
    tune_grid(
      grid = 20,
      resamples = cv_folds,
      control = control_grid(save_pred = TRUE,
                             save_workflow = TRUE),
      metrics = cls_metrics
    )
  saveRDS(lrl_decorr_log1p_res, resamples_file)
}

autoplot(lrl_decorr_log1p_res)
```

```{r}
show_best(lrl_decorr_log1p_res)
```

### The 12 best predictors

```{r}
# this will require a new data split
set.seed(124)

data12 <- data %>% 
  select(all_of(very_basic_predictors), SalePrice)

data12_split <- initial_split(data12, prop = 0.95)

df12_train <- training(data12_split)
df12_test <- testing(data12_split)

# and new folds
set.seed(124)
cv_folds_12 <- vfold_cv(df12_train, v = 10, repeats = 10) 

# and a new recipe
decorr12_log_recipe <- recipe(SalePrice ~ ., data = df12_train) %>%
  step_nzv(all_predictors()) %>%
  step_impute_mean(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_log(all_numeric_predictors(), offset = 1) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors(), threshold = tune("corr_tune")) %>%
  step_log("SalePrice")

resamples_file <- "data/lrl_decorr12_log1p_res_cv10.rds"

lrl_spec <- linear_reg(penalty = tune(),
                       mixture = tune()) %>%
  set_engine("glmnet", num.threads = 4) %>%
  set_mode("regression")

if (file.exists(resamples_file)) {
  print("Found resamples file!")
  lrl_decorr12_log1p_res <- readRDS(resamples_file)
} else {
  # model's workflow
  lrl_wf <- workflow() %>%
    add_model(lrl_spec) %>%
    add_recipe(decorr12_log_recipe)
  
  # tuning and resampling
  lrl_decorr12_log1p_res <- lrl_wf %>%
    tune_grid(
      grid = 20,
      resamples = cv_folds_12,
      control = control_grid(save_pred = TRUE,
                             save_workflow = TRUE),
      metrics = cls_metrics
    )
  saveRDS(lrl_decorr12_log1p_res, resamples_file)
}

autoplot(lrl_decorr12_log1p_res)
```

```{r}
show_best(lrl_decorr12_log1p_res)
```

### Comparison of mean  RMSE

```{r}
# function to retrieve the best HP combination based on mean RMSE
best_rmse <- function(res_obj, name){
  res_obj %>% 
    show_best("rmse") %>% 
    select(mean, std_err) %>%
    mutate(model = name) %>% 
    slice_head(n = 1)
}

# make a data frame
best_models_rmse <-
  map2_dfr(
    list(
      lrl_decorr_res,
      lrl_decorr_yj_res,
      lrl_decorr_log1p_res,
      lrl_decorr12_log1p_res
    ),
    c(
      "LRL decorr",
      "LRL yj",
      "LRL log",
      "LRL 12 log"
    ),
    ~ best_rmse(.x, .y)
  )

ggplot(best_models_rmse,
       aes(
         y = mean,
         x = model,
         ymin = mean - std_err,
         ymax = mean + std_err
       )) +
  geom_errorbar(aes(color = model)) +
  geom_point(aes(y = mean, color = model)) +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.95, hjust=1)) +
  xlab("") +
  ylab("RMSE")
```

```{r}
library(tidyposterior)

# function to get a name of the best preprocessor from your resample
best_preprocessor <- function(res_obj) {
  res_obj %>% 
    select_best("rmse") %>% 
    pull(.config)
}

# function to make a df with AUCs from the best preproc
best_rmse <- function(res_obj, model_name){
  res_obj %>%
    collect_metrics(summarize = FALSE) %>%
    filter(.metric == "rmse", .config == best_preprocessor(res_obj)) %>%
    select(id, id2, {{model_name}} := .estimate)
}

# generate distributions
mod_names <- c("LRL decorr",
               "LRL yj",
               "LRL log")

mod_resamples <- list(clearly
  lrl_decorr_res,
  lrl_decorr_yj_res,
  lrl_decorr_log1p_res
)

resampling_comparison <-
  map2(mod_resamples, mod_names, ~ best_rmse(res_obj = .x, model_name = .y)) %>%
  reduce(inner_join, by = c("id", "id2")) %>%
  set_names(c("id", "id2", mod_names)) %>%
  unite(id, id, id2)

# make ANOVA
resampling_posterior <-
  perf_mod(
    resampling_comparison,
    iter = 10000,
    seed = 100,
    refresh = 0,
    chains = 4,
    cores = 4
  )

# plot
autoplot(resampling_posterior)
```

```{r}
res_diff <- contrast_models(resampling_posterior, seed = 100) 

summary(res_diff)
```

## Basic RF

basic pre-processing recipe & space-filled grid search

```{r}
resamples_file <- "data/rf_base_res_cv10_new_split.rds"

# model specification
rf_spec <- rand_forest(mtry = tune(),
                      min_n = tune(),
                      trees = tune()) %>%
  set_engine("ranger", num.threads = 8) %>%
  set_mode("regression")

if (file.exists(resamples_file)) {
  print("Found resamples file")
  rf_base_res <- readRDS(resamples_file)
} else {
  # model workflow
  rf_wf <- workflow() %>%
    add_model(rf_spec) %>%
    add_recipe(base_recipe)
  
  # tuning and resampling
  rf_base_res <- rf_wf %>%
    tune_grid(
      grid = 20,
      resamples = cv_folds,
      control = control_grid(save_pred = TRUE,
                             save_workflow = TRUE),
      metrics = cls_metrics
    )
  saveRDS(rf_base_res, resamples_file)
}

autoplot(rf_base_res)
```

```{r}
rf_base_res %>% 
  show_best("rmse")
```

## Basic RF + log transformation of the target

to compare with LRs

```{r}
resamples_file <- "data/rf_base_log_res_cv10_new_split.rds"

# model specification
rf_spec <- rand_forest(mtry = tune(),
                      min_n = tune(),
                      trees = tune()) %>%
  set_engine("ranger", num.threads = 8) %>%
  set_mode("regression")

if (file.exists(resamples_file)) {
  print("Found resamples file")
  rf_base_log_res <- readRDS(resamples_file)
} else {
  # model workflow
  rf_wf <- workflow() %>%
    add_model(rf_spec) %>%
    add_recipe(base_log_recipe)
  
  # tuning and resampling
  rf_base_log_res <- rf_wf %>%
    tune_grid(
      grid = 20,
      resamples = cv_folds,
      control = control_grid(save_pred = TRUE,
                             save_workflow = TRUE),
      metrics = cls_metrics
    )
  saveRDS(rf_base_log_res, resamples_file)
}

autoplot(rf_base_log_res)
```

```{r}
rf_base_log_res %>% show_best()
```

## MLP

### Decorrelated predictors & YJ transformation

```{r}
mlp_nnet_spec <-
  mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %>%
  set_engine('nnet', num.threads = 8) %>%
  set_mode('regression')

resamples_file <- "data/mlp_decorr_yj_res_cv10_new_split.rds"

if (file.exists(resamples_file)) {
  print("Found resamples file")
  mlp_decorr_yj_res <- readRDS(resamples_file)
} else {
  mlp_wf <- workflow() %>%
    add_model(mlp_nnet_spec) %>%
    add_recipe(decorr_yj_recipe)
  
  mlp_decorr_yj_res <- mlp_wf %>%
    tune_grid(
      grid = 20,
      resamples = cv_folds,
      control = control_grid(save_pred = TRUE,
                             save_workflow = TRUE),
      metrics = cls_metrics
    )
  saveRDS(mlp_decorr_yj_res, resamples_file)
}

autoplot(mlp_decorr_yj_res)
```

#### Best models

```{r}
mlp_decorr_yj_res %>% 
  show_best()
```

```{r}
# make a data frame
best_models_rmse <-
  map2_dfr(
    list(
      lrl_decorr_res,
      lrl_decorr_yj_res,
      lrl_decorr_log1p_res,
      rf_base_log_res,
      mlp_decorr_yj_res
    ),
    c(
      "LRL norm log",
      "LRL yj log",
      "LRL log",
      "RF log",
      "MLP yj log"
    ),
    ~ best_rmse(.x, .y)
  )

ggplot(best_models_rmse,
       aes(
         y = mean,
         x = model,
         ymin = mean - std_err,
         ymax = mean + std_err
       )) +
  geom_errorbar(aes(color = model)) +
  geom_point(aes(y = mean, color = model)) +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.95, hjust=1)) +
  xlab("") +
  ylab("RMSE")
```

## XGB

### Boosted trees specification

```{r}
xgb_spec <- boost_tree(
  trees = 50,
  mtry = tune(),
  min_n = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune(),
  sample_size = tune(),
  stop_iter = tune()
) %>%
  set_engine("xgboost", num.threads = 8) %>%
  set_mode("regression")
```


### Basic preprocessing and win-loss grid search

```{r}
resamples_file <- "data/xgb_wl_res_cv5.rds" # the previous one was cv 10, with rmse 27.9 k

if (file.exists(resamples_file)) {
  print("Found resamples file")
  xgb_wl_res <- readRDS(resamples_file)
} else {
  xgb_wf <- workflow() %>%
    add_model(xgb_spec) %>%
    add_recipe(base_recipe)
  
  param_set <- extract_parameter_set_dials(xgb_wf) %>%
    finalize(x = df_train %>% select(-SalePrice))
  
  xgb_wl_res <- xgb_wf %>%
    tune_race_win_loss(
      param_info = param_set,
      resamples = cv_folds,
      grid = 40,
      control = control_race(
        verbose_elim = TRUE,
        save_pred = TRUE,
        save_workflow = TRUE,
        burn_in = 10
      )
    )
  
  saveRDS(xgb_wl_res, resamples_file)
}

plot_race(xgb_wl_res)
```

#### Best models

```{r}
xgb_wl_res %>% 
  show_best("rmse")
```


### Decorrelated recipe and win-loss search

```{r, eval=FALSE}
resamples_file <- "data/xgb_decorr_wl_res_cv5.rds"

if (file.exists(resamples_file)) {
  print("Found resamples file")
  xgb_decorr_wl_res <- readRDS(resamples_file)
} else {
  xgb_wf <- workflow() %>%
    add_model(xgb_spec) %>%
    add_recipe(decorr_recipe)
  
  param_set <- extract_parameter_set_dials(xgb_wf) %>%
    finalize(x = df_train %>% select(-SalePrice))
  
  xgb_decorr_wl_res <- xgb_wf %>%
    tune_race_win_loss(
      param_info = param_set,
      resamples = cv_folds,
      grid = 20,
      control = control_race(
        verbose_elim = TRUE,
        save_pred = TRUE,
        save_workflow = TRUE,
        burn_in = 10
      )
    )
  saveRDS(xgb_decorr_wl_res, resamples_file)
}

plot_race(xgb_decorr_wl_res)
```

#### Best models

```{r, eval=FALSE}
xgb_decorr_wl_res %>% 
  show_best("rmse")
```

### RFE and win-loss search

```{r}
resamples_file <- "data/xgb_rfe_wl_res_cv5.rds"

if (file.exists(resamples_file)) {
  print("Found resamples file")
  xgb_rfe_wl_res <- readRDS(resamples_file)
} else {
  # xgb specification is the same as above
  xgb_wf <- workflow() %>%
    add_model(xgb_spec) %>%
    add_recipe(base_rfe_recipe)
  
  param_set <- extract_parameter_set_dials(xgb_wf) %>%
    finalize(x = df_train %>% select(-SalePrice))
  
  xgb_rfe_wl_res <- xgb_wf %>%
    tune_race_win_loss(
      param_info = param_set,
      resamples = cv_folds,
      grid = 40,
      control = control_race(
        verbose_elim = TRUE,
        save_pred = TRUE,
        save_workflow = TRUE,
        burn_in = 10
      )
    )
  
  saveRDS(xgb_rfe_wl_res, resamples_file)
}

plot_race(xgb_rfe_wl_res)
```

#### Best models

```{r}
xgb_rfe_wl_res %>% 
  show_best("rmse")
```

### RFE and Bayesian search

```{r}
resamples_file <- "data/xgb_rfe_bres_cv5.rds"

if (file.exists(resamples_file)) {
  print("Found resamples file")
  xgb_rfe_bres <- readRDS(resamples_file)
} else {
  
  # xgb specification is the same as above
  xgb_wf <- workflow() %>%
    add_model(xgb_spec) %>%
    add_recipe(base_rfe_recipe)
  
  param_set <- extract_parameter_set_dials(xgb_wf) %>%
    finalize(x = df_train %>% select(-SalePrice))
  
   xgb_rfe_bres <- xgb_wf %>% 
     tune_bayes(
      resamples = cv_folds,
      param_info = param_set,
      initial = 30,
      iter = 20,
      metrics = cls_metrics,
      control = control_bayes(
        no_improve = 20,
        verbose = FALSE,
        save_pred = TRUE,
        save_workflow = TRUE
      )
    )

  saveRDS(xgb_rfe_bres, resamples_file)
}

autoplot(xgb_rfe_bres)
```

#### Best models

```{r}
xgb_rfe_bres %>% show_best("rmse")
```


### Basic recipe and full Bayesian search

30 iterations

```{r}
resamples_file <- "data/xgb_base_bres_cv5.rds"

if (file.exists(resamples_file)) {
  print("Found resamples file")
  xgb_base_bres <- readRDS(resamples_file)
} else {
  xgb_wf <- workflow() %>%
    add_model(xgb_spec) %>%
    add_recipe(base_recipe)
  
  param_set <- extract_parameter_set_dials(xgb_wf) %>%
    finalize(x = df_train %>% select(-SalePrice))
  
  xgb_base_bres <- xgb_wf %>%
    tune_bayes(
      resamples = cv_folds,
      param_info = param_set,
      initial = 20,
      iter = 30,
      metrics = cls_metrics,
      control = control_bayes(
        no_improve = 20,
        verbose = FALSE,
        save_pred = TRUE,
        save_workflow = TRUE
      )
    )
  
  saveRDS(xgb_base_bres, resamples_file)
  
}

autoplot(xgb_base_bres)
```

#### Autoplot

```{r}
xgb_base_bres %>% 
  show_best("rmse")
```


### Decorrelated recipe and Bayesian search

30 iterations

```{r}
resamples_file <- "data/xgb_dec_bres_cv5.rds"

if (file.exists(resamples_file)) {
  print("Found resamples file")
  xgb_dec_bres <- readRDS(resamples_file)
} else {
  xgb_wf <- workflow() %>%
    add_model(xgb_spec) %>%
    add_recipe(decorr_recipe)
  
  param_set <- extract_parameter_set_dials(xgb_wf) %>%
    finalize(x = df_train %>% select(-SalePrice))
  
  xgb_dec_bres <- xgb_wf %>%
    tune_bayes(
      resamples = cv_folds,
      param_info = param_set,
      initial = 20,
      iter = 30,
      metrics = cls_metrics,
      control = control_bayes(
        no_improve = 20,
        verbose = FALSE,
        save_pred = TRUE,
        save_workflow = TRUE
      )
    )
  saveRDS(xgb_dec_bres, resamples_file)
}

autoplot(xgb_dec_bres)
```

#### Autoplot

```{r, fig.width=7}
xgb_dec_bres %>% 
  show_best("rmse")
```


## Compare mean RMSE between different model specifications

```{r}
# function to retrieve the best HP combination based on mean RMSE
best_rmse <- function(res_obj, name){
  res_obj %>% 
    show_best("rmse") %>% 
    select(mean, std_err) %>%
    mutate(model = name) %>% 
    slice_head(n = 1)
}

# make a data frame
best_models_rmse <-
  map2_dfr(
    list(
      lrl_decorr_res,
      xgb_base_bres,
      xgb_wl_res,
      xgb_rfe_wl_res,
      xgb_dec_bres
    ),
    c(
      "LRL decorr",
      "XGB base",
      "XGB wl",
      "XGB rfe wl",
      "XGB dec"
    ),
    ~ best_rmse(.x, .y)
  )

ggplot(best_models_rmse,
       aes(
         y = mean,
         x = model,
         ymin = mean - std_err,
         ymax = mean + std_err
       )) +
  geom_errorbar(aes(color = model)) +
  geom_point(aes(y = mean, color = model)) +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.95, hjust=1)) +
  xlab("") +
  ylab("RMSE")
```

### Past-hoc Bayesian ANOVA

```{r}
library(tidyposterior)

# function to get a name of the best preprocessor from your resample
best_preprocessor <- function(res_obj) {
  res_obj %>% 
    select_best("rmse") %>% 
    pull(.config)
}

# function to make a df with AUCs from the best preproc
best_rmse <- function(res_obj, model_name){
  res_obj %>%
    collect_metrics(summarize = FALSE) %>%
    filter(.metric == "rmse", .config == best_preprocessor(res_obj)) %>%
    select(id, id2, {{model_name}} := .estimate)
}

# generate distributions
mod_names <- c("LRL decorr",
               "RF base",
               "XGB base",
               "XGB wl",
               "XGB rfe wl",
               "XGB dec")

mod_resamples <- list(
  lrl_decorr_res,
  rf_base_res,
  xgb_base_bres,
  xgb_wl_res,
  xgb_rfe_wl_res,
  xgb_dec_bres
)

resampling_comparison <-
  map2(mod_resamples, mod_names, ~ best_rmse(res_obj = .x, model_name = .y)) %>%
  reduce(inner_join, by = c("id", "id2")) %>%
  set_names(c("id", "id2", mod_names)) %>%
  unite(id, id, id2)

# make ANOVA
resampling_posterior <-
  perf_mod(
    resampling_comparison,
    iter = 10000,
    seed = 100,
    refresh = 0,
    chains = 4,
    cores = 4
  )

# plot
autoplot(resampling_posterior)
```

### Estimate the difference

```{r}
res_diff <- contrast_models(resampling_posterior, seed = 100) 

summary(res_diff)
```

# Final fit

 emulates the process where, after determining the best model, 
 the final fit on the entire training set is needed 
 and is then evaluated on the test set.
 
 
## XGB base + Bayes

```{r}
xgb_spec <- boost_tree(
  trees = 50,
  mtry = tune(),
  min_n = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune(),
  sample_size = tune(),
  stop_iter = tune()
) %>%
  set_engine("xgboost", num.threads = 8) %>%
  set_mode("regression")

# best model
best_mod <- xgb_base_bres %>% 
  select_best("rmse")

# use hyper parameters of the best model
last_mod <-
  boost_tree(
    mtry = best_mod$mtry,
    min_n = best_mod$min_n,
    tree_depth = best_mod$tree_depth,
    learn_rate = best_mod$learn_rate,
    loss_reduction = best_mod$loss_reduction,
    sample_size = best_mod$sample_size,
    stop_iter = best_mod$stop_iter
  ) %>% 
  set_engine("xgboost", num.threads=8) %>% 
  set_mode("regression") 

# update corr threshold in the chosen recipe
# decorr_recipe$steps[[6]] <-
#   recipes::update(decorr_recipe$steps[[6]], threshold = best_mod$corr_tune)

xgb_wf <- workflow() %>% 
  add_model(xgb_spec) %>% 
  add_recipe(base_recipe) # the recipe here should be the one that was used for this model

# the last workflow
last_wf <- 
  xgb_wf %>%  
  update_model(last_mod)

# the last fit
set.seed(345)

final_fit <- 
  last_wf %>% 
  last_fit(data_split)


final_fit %>% 
  collect_metrics()
```


## XGB RFE + Bayes

```{r}
# best model
best_mod <- xgb_rfe_bres %>% 
  select_best("rmse")

# use hyper parameters of the best model
last_mod <-
  boost_tree(
    mtry = best_mod$mtry,
    min_n = best_mod$min_n,
    tree_depth = best_mod$tree_depth,
    learn_rate = best_mod$learn_rate,
    loss_reduction = best_mod$loss_reduction,
    sample_size = best_mod$sample_size,
    stop_iter = best_mod$stop_iter
  ) %>% 
  set_engine("xgboost", num.threads=8) %>% 
  set_mode("regression") 

xgb_wf <- workflow() %>% 
  add_model(xgb_spec) %>% 
  add_recipe(base_rfe_recipe) # the recipe here should be the one that was used for this model

# the last workflow
last_wf <- 
  xgb_wf %>%  
  update_model(last_mod)

# the last fit
set.seed(345)

final_fit <- 
  last_wf %>% 
  last_fit(data_split)


final_fit %>% 
  collect_metrics()
```


## LRL + YJ and log

```{r}

# best model
best_mod <- lrl_decorr_yj_res %>% 
  select_best("rmse")

# use hyper parameters of the best model
last_mod <-
  linear_reg(penalty = best_mod$penalty, mixture = best_mod$mixture) %>%
  set_engine("glmnet", num.threads = 2) %>%
  set_mode("regression")

# update corr threshold in a copy of the chosen recipe
decorr_yj_recipe_tuned <- decorr_yj_recipe
decorr_yj_recipe_tuned$steps[[8]] <-
  recipes::update(decorr_yj_recipe_tuned$steps[[8]], threshold = best_mod$corr_tune)

lrl_wf <- workflow() %>% 
  add_model(lrl_spec) %>% 
  add_recipe(decorr_yj_recipe_tuned) # the recipe here should be the one that was used for this model

# the last workflow
last_wf <- 
  lrl_wf %>%  
  update_model(last_mod)

# the last fit
set.seed(34)

final_fit <- 
  last_wf %>% 
  last_fit(data_split)


final_fit %>% 
  collect_metrics()
```


## LRL + log

```{r}
# best model
best_mod <- lrl_decorr_log1p_res %>% 
  select_best("rmse")

# use hyper parameters of the best model
last_mod <-
  linear_reg(penalty = best_mod$penalty, mixture = best_mod$mixture) %>%
  set_engine("glmnet", num.threads = 2) %>%
  set_mode("regression")

# update corr threshold in a copy of the chosen recipe
decorr_log_recipe_tuned <- decorr_log_recipe
decorr_log_recipe_tuned$steps[[8]] <-
  recipes::update(decorr_log_recipe_tuned$steps[[8]], threshold = best_mod$corr_tune)

lrl_wf <- workflow() %>% 
  add_model(lrl_spec) %>% 
  add_recipe(decorr_log_recipe_tuned) # the recipe here should be the one that was used for this model

# the last workflow
last_wf <- 
  lrl_wf %>%  
  update_model(last_mod)

# the last fit
set.seed(34)

final_fit <- 
  last_wf %>% 
  last_fit(data_split)

final_fit %>% 
  collect_metrics()
```


# Prediction

```{r}
data_submission <- read_csv("data/test.csv", show_col_types = FALSE) 

# same changes as in the main data set
data_submission <- data_submission %>%
  mutate_at(
    vars(
      Alley,
      BsmtQual,
      BsmtCond,
      BsmtExposure,
      BsmtFinType1,
      BsmtFinType2,
      FireplaceQu,
      GarageType,
      GarageCond,
      GarageFinish,
      GarageQual,
      PoolQC,
      Fence
    ),
    ~ replace_na(., "No")
  ) %>% 
mutate(across(is_character, ~ as_factor(.x)))

data_submission <- data_submission %>% 
  mutate(MSSubClass = as.factor(MSSubClass))

# fake SalePrice to match the number of columns
data_submission$SalePrice <- 0

final_train_fit <- 
  last_wf %>% 
  fit(df_train)

final_recipe <-
  final_train_fit %>%
  extract_recipe()

final_mod_fit <- 
  final_train_fit %>% 
  extract_fit_parsnip()

data_submission_processed <- 
  final_recipe %>% 
  bake(new_data = data_submission) 

prediction_data_submission <-
  predict(final_mod_fit, new_data = data_submission_processed)

pred_submission_df <- tibble(Id = data_submission$Id, 
                             SalePrice = exp(prediction_data_submission$.pred))
pred_submission_df
```

- the first submission 0.17780 / place 3682 out of 4672

- LRL YJ input - log target: 0.12912 / place 959 (899 excluding cheaters)

- LRL log input - log target:  0.12719 / place 812 (752 excluding cheaters)

- 

```{r}

write_csv(pred_submission_df, "data/prediction_submission_log1p_mssub.csv", quote = "none")
```

